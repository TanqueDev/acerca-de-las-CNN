{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f4d9055",
   "metadata": {},
   "source": [
    "\n",
    "<h2 align=\"center\"><strong> Redes Neuronales Convolucionales (CNN) utilizando Python</strong></h2>\n",
    "\n",
    "\n",
    "Por: Joan Esteban López Narváez, Semillero ARES - Grupo GADyM, Universidad del Valle.\n",
    "Contacto: joan.narvaez@correounivalle.edu.co\n",
    "\n",
    "Versión 1.0\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"red.png\" alt=\"Diagrama de la arquitectura de una red neuronal convolucional\" style=\"max-width:40%; height:auto;\">\n",
    "</p>\n",
    "\n",
    "Imagen tomada de : https://www.youtube.com/watch?v=dEXPMQXoiLc&list=PLQVvvaa0QuDcjD5BAw2DxE6OF2tius3V3&index=7\n",
    "\n",
    "\n",
    "En las siguientes secciones se presentarán, de forma gradual, los conceptos clave para comprender las redes neuronales convolucionales (CNN): neuronas, optimización, retropropagación (backpropagation) y otros temas relacionados. Cada concepto se construirá paso a paso de manera manual para afianzar la intuición, y posteriormente se mostrará cómo aprovechar librerías como TensorFlow y NumPy para implementar y optimizar arquitecturas CNN.\n",
    "\n",
    "Si se busca más profundidad, se pueden afianzar los conceptos usando fuentes como lo son el Deep Learning Book, de Yoshua Bengio et Al., de acceso libre en el siguiente link: https://www.deeplearningbook.org/\n",
    "\n",
    "Además, un buen recurso visual se encuentra en el siguiente curso corto (en el que se basó el contenido principal del libro computacional): https://www.youtube.com/watch?v=omz_NdFgWyU&list=PLQVvvaa0QuDcjD5BAw2DxE6OF2tius3V3&index=6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a135defa",
   "metadata": {},
   "source": [
    "## Tabla de Contenido\n",
    "\n",
    "- [Introduccion](#introduccion)\n",
    "- [Codificacion de una capa](#codificacion-de-una-capa)\n",
    "- [Producto Escalar](#producto-escalar)\n",
    "- [Lotes, capas y objetos](#lotes-capas-y-objetos)\n",
    "- [Funciones de activacion](#funciones-de-activacion)\n",
    "- [Activacion softmax](#activacion-softmax)\n",
    "- [Calculo de perdidas](#calculo-de-perdidas)\n",
    "- [Optimizacion](#optimizacion)\n",
    "- [Implementacion en Tensorflow](#Implementacion-en-Tensorflow)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfdea2f",
   "metadata": {},
   "source": [
    "## Introduccion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd637001-5f8b-4b33-b3da-8e461d99c249",
   "metadata": {},
   "source": [
    "Las **redes neuronales convolucionales (CNN, *Convolutional Neural Networks*)** constituyen una de las arquitecturas más influyentes dentro del campo del *deep learning* y el procesamiento de información visual.  \n",
    "\n",
    "Su éxito se debe a la capacidad de **extraer automáticamente características jerárquicas** de los datos, lo que les permite reconocer patrones complejos en imágenes, señales o secuencias sin requerir una etapa explícita de ingeniería de características.\n",
    "\n",
    "El principio fundamental de las CNN radica en el uso de **operaciones de convolución**, las cuales actúan como filtros capaces de detectar estructuras locales tales como bordes, texturas o regiones específicas de interés. Al combinar múltiples capas convolucionales, seguidas de operaciones de activación y reducción de dimensionalidad, la red puede construir representaciones cada vez más abstractas y robustas del conjunto de entrada.\n",
    "\n",
    "Históricamente, las CNN surgieron como una evolución de los modelos neuronales clásicos inspirados en la organización del **córtex visual biológico**, y ganaron popularidad tras el éxito de **LeNet-5** (LeCun et al., 1998) en el reconocimiento de dígitos manuscritos. A partir de entonces, arquitecturas más profundas como **AlexNet**, **VGG**, **ResNet** e **Inception** impulsaron de manera decisiva el progreso en tareas de visión por computador, extendiendo su aplicación a otros dominios como el análisis de señales biomédicas, la visión robótica o el procesamiento de audio.\n",
    "\n",
    "En este cuaderno se abordarán, de forma progresiva, los **conceptos fundamentales necesarios para comprender y construir una CNN desde cero**. Se explorarán las nociones de **neuronas, funciones de activación, optimización y retropropagación (backpropagation)**, entre otros elementos esenciales.  \n",
    "Cada tema será desarrollado **paso a paso y de manera manual** para reforzar la intuición matemática y computacional, y posteriormente se mostrará cómo **implementar estos mismos principios utilizando librerías especializadas como NumPy y TensorFlow**, facilitando el diseño y entrenamiento de arquitecturas convolucionales modernas.\n",
    "\n",
    "### Fundamentos de una red neuronal artificial  \n",
    "\n",
    "Una **red neuronal artificial (ANN)** es un modelo computacional inspirado en el funcionamiento del cerebro humano. Su estructura básica está formada por **neuronas artificiales**, unidades matemáticas que procesan información. Cada neurona recibe un conjunto de **entradas** $x_1, x_2, \\ldots, x_n$, las **multiplica por pesos** $w_1, w_2, \\ldots, w_n$ (es decir, realiza productos $w_i x_i$), suma los resultados y aplica una **función de activación** $f(\\cdot)$, produciendo una salida\n",
    "\n",
    "$$\n",
    "y = f\\!\\left(\\sum_i w_i x_i + b\\right)\n",
    "$$\n",
    "\n",
    "donde $b$ es un **sesgo**.\n",
    "\n",
    "Este proceso puede parecer matemáticamente complejo, pero no es nada más que un simple ciclo for en el que, en una variable, se guarda la suma de la multiplicación de cada peso por cada una de las entradas dadas, tal y como se evidencia a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0e88724-7ce1-4c12-83c9-4de58db67e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la salida de la neurona es 9.8\n"
     ]
    }
   ],
   "source": [
    "pesos = [0.1,2,3.5]\n",
    "entradas = [1,2,3]\n",
    "sesgo = 5.7\n",
    "salida = 0\n",
    "\n",
    "for i in range(2):\n",
    "    salida = salida + pesos[i] * entradas[i]\n",
    "\n",
    "salida = salida + sesgo\n",
    "\n",
    "print(f'la salida de la neurona es {salida}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084b6824-1bff-4180-b495-4b5baa9af019",
   "metadata": {},
   "source": [
    "Las neuronas se organizan en **capas**, que no son nada más ni nada menos que una **agrupación de neuronas**, cada una con sus propios pesos asignados:  \n",
    "- La **capa de entrada** recibe los datos sin procesar (por ejemplo, los píxeles de una imagen).  \n",
    "- Las **capas ocultas** realizan transformaciones lineales (multiplicación por pesos y suma) seguidas de no linealidades (activaciones). Es en estas capas donde se extraen y combinan características: los **pesos** son exactamente los parámetros que determinan cómo se valoran las entradas y **son los que se ajustan** durante el entrenamiento para que la red aprenda.  \n",
    "- La **capa de salida** genera la predicción final; suele tener **n neuronas** (una por clase), y la neurona con la mayor activación indica la **decisión final** del modelo.\n",
    "\n",
    "El proceso de entrenamiento busca ajustar esos pesos (y sesgos) minimizando una función de pérdida entre la salida predicha y la verdad de referencia, de modo que la red mejore su capacidad para reconocer patrones. A continuación, se presenta una lúdica interactiva que permite variar los pesos mediante un slider para ver el cambio en la salida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b27ed94-4adc-405e-91ee-3a5a4e1a9f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "904c657e50ed47f79f8046ce27c19c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='Mult. Peso 0:', max=1.0, min=-1.0, step=0.05), Float…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.calcular_y_visualizar(mult_peso0, mult_peso1, mult_peso2)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Valores originales\n",
    "pesos_originales = [0.1, 2, 3.5]\n",
    "entradas = [1, 2, 3]\n",
    "sesgo = 5.7\n",
    "\n",
    "def calcular_y_visualizar(mult_peso0, mult_peso1, mult_peso2):\n",
    "    \"\"\"Calcula y visualiza la salida de la neurona\"\"\"\n",
    "    pesos_modificados = [\n",
    "        pesos_originales[0] * mult_peso0,\n",
    "        pesos_originales[1] * mult_peso1,\n",
    "        pesos_originales[2] * mult_peso2\n",
    "    ]\n",
    "    \n",
    "    # Calcular contribuciones\n",
    "    contribuciones = [pesos_modificados[i] * entradas[i] for i in range(len(pesos_modificados))]\n",
    "    salida = sum(contribuciones) + sesgo\n",
    "    \n",
    "    # Crear visualización\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Gráfica 1: Contribuciones por peso\n",
    "    colores = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "    bars1 = ax1.bar(range(len(contribuciones)), contribuciones, color=colores, alpha=0.7)\n",
    "    ax1.axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    ax1.set_xlabel('Índice del Peso', fontsize=12)\n",
    "    ax1.set_ylabel('Contribución a la Salida', fontsize=12)\n",
    "    ax1.set_title('Contribución de cada Peso × Entrada', fontsize=14, fontweight='bold', pad=20)\n",
    "    ax1.set_xticks(range(len(contribuciones)))\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Calcular límites y-axis con margen para las etiquetas\n",
    "    max_val = max(contribuciones) if contribuciones else 0\n",
    "    min_val = min(contribuciones) if contribuciones else 0\n",
    "    rango = max_val - min_val\n",
    "    margen = rango * 0.15 if rango > 0 else 1\n",
    "    ax1.set_ylim(min_val - margen, max_val + margen)\n",
    "    \n",
    "    # Añadir valores en las barras\n",
    "    for i, v in enumerate(contribuciones):\n",
    "        offset = 0.3 if abs(v) > 0.5 else 0.15\n",
    "        ax1.text(i, v + offset if v > 0 else v - offset, f'{v:.2f}', \n",
    "                ha='center', va='bottom' if v > 0 else 'top', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    # Gráfica 2: Composición de la salida\n",
    "    componentes = contribuciones + [sesgo]\n",
    "    labels = [f'Peso {i}×Entrada {i}' for i in range(len(contribuciones))] + ['Sesgo']\n",
    "    colores_comp = colores + ['#95E1D3']\n",
    "    \n",
    "    bars2 = ax2.barh(labels, componentes, color=colores_comp, alpha=0.7)\n",
    "    ax2.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    ax2.set_xlabel('Valor', fontsize=12)\n",
    "    ax2.set_title(f'Composición de la Salida = {salida:.3f}', fontsize=14, fontweight='bold', pad=20)\n",
    "    ax2.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Calcular límites x-axis con margen para las etiquetas\n",
    "    max_comp = max(componentes) if componentes else 0\n",
    "    min_comp = min(componentes) if componentes else 0\n",
    "    rango_comp = max_comp - min_comp\n",
    "    margen_comp = rango_comp * 0.2 if rango_comp > 0 else 1\n",
    "    ax2.set_xlim(min_comp - margen_comp, max_comp + margen_comp)\n",
    "    \n",
    "    # Añadir valores\n",
    "    for i, v in enumerate(componentes):\n",
    "        offset = 0.5 if abs(v) > 1 else 0.3\n",
    "        ax2.text(v + offset if v > 0 else v - offset, i, f'{v:.2f}', \n",
    "                va='center', ha='left' if v > 0 else 'right', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout(pad=2.0)\n",
    "    plt.show()\n",
    "    \n",
    "    # Mostrar información numérica\n",
    "    print(\"=\" * 60)\n",
    "    print(f\" SALIDA FINAL DE LA NEURONA: {salida:.3f}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Pesos modificados: [{pesos_modificados[0]:.3f}, {pesos_modificados[1]:.3f}, {pesos_modificados[2]:.3f}]\")\n",
    "    print(f\"Multiplicadores:   [{mult_peso0:.2f}, {mult_peso1:.2f}, {mult_peso2:.2f}]\")\n",
    "\n",
    "# Crear sliders\n",
    "slider_peso0 = widgets.FloatSlider(value=1.0, min=-1.0, max=1.0, step=0.05, \n",
    "                                   description='Mult. Peso 0:', continuous_update=True)\n",
    "slider_peso1 = widgets.FloatSlider(value=1.0, min=-1.0, max=1.0, step=0.05, \n",
    "                                   description='Mult. Peso 1:', continuous_update=True)\n",
    "slider_peso2 = widgets.FloatSlider(value=1.0, min=-1.0, max=1.0, step=0.05, \n",
    "                                   description='Mult. Peso 2:', continuous_update=True)\n",
    "\n",
    "# Crear interfaz interactiva\n",
    "widgets.interact(calcular_y_visualizar,\n",
    "                mult_peso0=slider_peso0,\n",
    "                mult_peso1=slider_peso1,\n",
    "                mult_peso2=slider_peso2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b60183a",
   "metadata": {},
   "source": [
    "## Codificacion de una capa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec268fc2-9d02-4d40-9f55-6547877f603d",
   "metadata": {},
   "source": [
    "Tras lo expuesto anteriormente, es natural que surjan algunas preguntas: ¿cómo se organizan realmente esas agrupaciones de capas de neuronas?, y más aún, ¿cómo pueden implementarse en Python, considerando que son las responsables del proceso de inferencia una vez reciben una entrada específica?\n",
    "A lo largo de este capítulo se abordarán estas cuestiones mediante la implementación paso a paso de una red neuronal de una sola capa, con el objetivo de comprender en detalle su funcionamiento interno.\n",
    "\n",
    "## Red neuronal de una capa oculta  \n",
    "\n",
    "Hasta ahora se ha visto cómo una única neurona combina varias entradas ponderadas por sus pesos, suma un sesgo y produce una salida. Sin embargo, en la práctica, las redes neuronales constan de **múltiples neuronas organizadas en capas**, donde la **salida de una capa sirve como entrada para la siguiente**.  \n",
    "\n",
    "Para comprender mejor esta estructura, construiremos una pequeña red con:\n",
    "- **4 entradas**,  \n",
    "- **una capa oculta con 3 neuronas**, y  \n",
    "- **una capa de salida con una única neurona**.\n",
    "\n",
    "El flujo de información será el siguiente:\n",
    "\n",
    "$$\n",
    "\\text{Entradas} \\rightarrow \\text{Capa oculta (3 neuronas)} \\rightarrow \\text{Capa de salida (1 neurona)}\n",
    "$$\n",
    "\n",
    "Cada neurona en la capa oculta calcula una salida independiente según:\n",
    "\n",
    "$$\n",
    "y_j = \\sum_{i=1}^{4} (x_i \\cdot w_{ij}) + b_j\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- $x_i$ son las entradas,  \n",
    "- $w_{ij}$ son los pesos que conectan la entrada $i$ con la neurona $j$,  \n",
    "- $b_j$ es el sesgo de la neurona $j$.\n",
    "\n",
    "Posteriormente, la **neurona de salida** tomará esas tres salidas de la capa oculta como sus entradas, aplicando el mismo principio.\n",
    "\n",
    "---\n",
    "\n",
    "### Implementación paso a paso en Python\n",
    "\n",
    "A continuación se muestra el código para calcular la salida final de la red (sin usar librerías externas):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22a252af-07d7-4a6d-9b8c-9235a199b6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salidas de la capa oculta: [4.8, 1.21, 2.385]\n",
      "\n",
      "Salida final de la red: 4.6060\n"
     ]
    }
   ],
   "source": [
    "# Entradas de la red (4 valores)\n",
    "\n",
    "entradas = [1.0, 2.0, 3.0, 2.5]\n",
    "\n",
    "# Pesos de la capa oculta (3 neuronas × 4 entradas)\n",
    "\n",
    "pesos_capa_oculta = [\n",
    "\n",
    "    [0.2, 0.8, -0.5, 1.0],   # Neurona 1\n",
    "\n",
    "    [0.5, -0.91, 0.26, -0.5],# Neurona 2\n",
    "\n",
    "    [-0.26, -0.27, 0.17, 0.87]# Neurona 3\n",
    "\n",
    "]\n",
    "\n",
    "# Sesgos de la capa oculta (uno por neurona)\n",
    "\n",
    "sesgos_capa_oculta = [2.0, 3.0, 0.5]\n",
    "\n",
    "# Cálculo de la salida de la capa oculta\n",
    "\n",
    "salidas_ocultas = []\n",
    "\n",
    "for i in range(3):  # tres neuronas\n",
    "\n",
    "    salida_neurona = 0\n",
    "\n",
    "    for j in range(4):  # cuatro entradas\n",
    "\n",
    "        salida_neurona += entradas[j] * pesos_capa_oculta[i][j]\n",
    "\n",
    "    salida_neurona += sesgos_capa_oculta[i]\n",
    "\n",
    "    salidas_ocultas.append(salida_neurona)\n",
    "\n",
    "print(\"Salidas de la capa oculta:\", salidas_ocultas)\n",
    "\n",
    "# -------------------------------\n",
    "\n",
    "# Capa de salida (una sola neurona)\n",
    "\n",
    "# -------------------------------\n",
    "\n",
    "pesos_salida = [0.3, -0.2, 0.8]  # tres pesos, uno por salida oculta\n",
    "\n",
    "sesgo_salida = 1.5\n",
    "\n",
    "# Cálculo de la salida final\n",
    "\n",
    "salida_final = 0\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    salida_final += salidas_ocultas[i] * pesos_salida[i]\n",
    "\n",
    "salida_final += sesgo_salida\n",
    "\n",
    "print(f\"\\nSalida final de la red: {salida_final:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236b701b",
   "metadata": {},
   "source": [
    "## Producto Escalar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb069543-ff49-4bc3-b862-72eda5aff83f",
   "metadata": {},
   "source": [
    "Una manera de optimizar las operaciones realizadas anteriormente consiste en representar los pesos y bias de cada una de las neuronas como vectores (que son listas de elementos), matrices (que son listas de múltiples elementos organizados en filas y columnas) o tensores (que pueden entenderse como una extensión de las matrices a más de dos dimensiones; por ejemplo, un tensor de tres dimensiones puede imaginarse como un conjunto de matrices apiladas una sobre otra, y un tensor de cuatro dimensiones como un conjunto de esos bloques a lo largo de una nueva dimensión).\n",
    "\n",
    "De esta manera, al utilizar el producto punto y, de manera más general, el producto entre matrices, es posible expresar los cálculos neuronales de forma compacta y eficiente, evitando el uso de múltiples bucles y reduciendo considerablemente el tiempo de ejecución.\n",
    "\n",
    "Para ello, se utilizará la librería NumPy de Python, la cual ofrece funciones altamente optimizadas para realizar operaciones vectoriales y matriciales. Gracias a esta herramienta, el proceso de cálculo de la salida de una neurona (o incluso de una capa completa) puede representarse en apenas una línea de código, mejorando tanto la legibilidad como el rendimiento del programa.\n",
    "\n",
    "En esencia, lo que intentamos calcular para encontrar la salida de una neurona (o de una capa completa) puede expresarse de manera general como:  \n",
    "\n",
    "$$\n",
    "\\text{salida} = \\text{entradas} \\times \\text{pesos} + \\text{bias}\n",
    "$$\n",
    "\n",
    "donde el término $\\text{entradas} \\times \\text{pesos}$ representa el **producto punto** entre ambos vectores.  \n",
    "Esta operación resume el proceso de multiplicar cada entrada por su peso correspondiente y luego sumar todos los resultados.  \n",
    "A este valor se le añade el **bias**, que permite ajustar la salida desplazando el resultado final.  \n",
    "Gracias a **NumPy**, este cálculo puede implementarse en una sola línea, como se muestra a continuación para el ejemplo inicial de una única neurona:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4215d4ad-7a8b-4db5-a6bc-f308f9aa5899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La salida de la neurona es 20.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Definición de entradas, pesos y bias\n",
    "entradas = np.array([1, 2, 3])\n",
    "pesos = np.array([0.1, 2, 3.5])\n",
    "bias = 5.7\n",
    "\n",
    "# Cálculo de la salida de la neurona\n",
    "salida = np.dot(entradas, pesos) + bias\n",
    "\n",
    "print(f'La salida de la neurona es {salida}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c5ca22-076c-4769-8494-3678f822bf76",
   "metadata": {},
   "source": [
    "Otro ejemplo un poco más visual del proceso que se lleva a cabo es el siguiente. Abajo se pueden ajustar los pesos de cada capa y visualizar gráficamente que es lo que está ocurriendo en cada paso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d673c300-2363-4f1e-af22-ef09b4eab088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ajusta los multiplicadores para ver cómo cambian las salidas de la red neuronal\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117c570d32114957b344b3bd6c280714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='Mult. Pesos Oculta:', max=1.0, min=-1.0, step=0.05, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.calcular_y_visualizar(mult_oculta, mult_salida)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Configuración de la red\n",
    "entradas = np.array([1.0, 2.0, 3.0])\n",
    "\n",
    "# Pesos y sesgos de la capa oculta (3 neuronas)\n",
    "pesos_oculta_originales = np.array([\n",
    "    [0.2, 0.8, -0.5],   # Pesos para neurona oculta 0\n",
    "    [0.5, -0.91, 0.26], # Pesos para neurona oculta 1\n",
    "    [-0.26, -0.27, 0.17] # Pesos para neurona oculta 2\n",
    "])\n",
    "sesgos_oculta = np.array([2.0, 3.0, 0.5])\n",
    "\n",
    "# Pesos y sesgos de la capa de salida (2 neuronas)\n",
    "pesos_salida_originales = np.array([\n",
    "    [0.1, 0.14, -0.5],  # Pesos para neurona salida 0\n",
    "    [-0.3, 0.2, 0.9]    # Pesos para neurona salida 1\n",
    "])\n",
    "sesgos_salida = np.array([-1.0, 1.0])\n",
    "\n",
    "def calcular_y_visualizar(mult_oculta, mult_salida):\n",
    "    \"\"\"Calcula y visualiza el flujo de información en la red neuronal\"\"\"\n",
    "    \n",
    "    # Aplicar multiplicadores a los pesos\n",
    "    pesos_oculta = pesos_oculta_originales * mult_oculta\n",
    "    pesos_salida = pesos_salida_originales * mult_salida\n",
    "    \n",
    "    # CAPA OCULTA: Calcular salidas de las 3 neuronas ocultas\n",
    "    salidas_oculta = np.array([\n",
    "        np.dot(entradas, pesos_oculta[0]) + sesgos_oculta[0],\n",
    "        np.dot(entradas, pesos_oculta[1]) + sesgos_oculta[1],\n",
    "        np.dot(entradas, pesos_oculta[2]) + sesgos_oculta[2]\n",
    "    ])\n",
    "    \n",
    "    # CAPA DE SALIDA: Calcular salidas de las 2 neuronas de salida\n",
    "    salidas_finales = np.array([\n",
    "        np.dot(salidas_oculta, pesos_salida[0]) + sesgos_salida[0],\n",
    "        np.dot(salidas_oculta, pesos_salida[1]) + sesgos_salida[1]\n",
    "    ])\n",
    "    \n",
    "    # Crear visualización\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    gs = fig.add_gridspec(3, 2, hspace=0.4, wspace=0.3)\n",
    "    \n",
    "    # --- GRÁFICA 1: Arquitectura de la red ---\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    ax1.set_xlim(0, 10)\n",
    "    ax1.set_ylim(0, 10)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('Arquitectura de la Red Neuronal', fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Dibujar conexiones primero (para que queden detrás)\n",
    "    # Conexiones entrada → oculta\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            ax1.plot([1.9, 4.6], [7 - i*2.5, 7 - j*2.5], color='#FFA500', alpha=0.5, linewidth=1.5)\n",
    "    # Conexiones oculta → salida\n",
    "    for i in range(3):\n",
    "        for j in range(2):\n",
    "            ax1.plot([5.4, 8.1], [7 - i*2.5, 6 - j*3], color='#9370DB', alpha=0.5, linewidth=1.5)\n",
    "    \n",
    "    # Dibujar capas encima de las conexiones\n",
    "    # Capa de entrada\n",
    "    for i in range(3):\n",
    "        circle = plt.Circle((1.5, 7 - i*2.5), 0.4, color='#FF6B6B', alpha=0.8, zorder=3)\n",
    "        ax1.add_patch(circle)\n",
    "        ax1.text(1.5, 7 - i*2.5, f'x{i}', ha='center', va='center', fontweight='bold', fontsize=10, zorder=4)\n",
    "        ax1.text(0.3, 7 - i*2.5, f'{entradas[i]:.1f}', ha='center', va='center', fontsize=9)\n",
    "    \n",
    "    # Capa oculta\n",
    "    for i in range(3):\n",
    "        circle = plt.Circle((5, 7 - i*2.5), 0.4, color='#4ECDC4', alpha=0.8, zorder=3)\n",
    "        ax1.add_patch(circle)\n",
    "        ax1.text(5, 7 - i*2.5, f'h{i}', ha='center', va='center', fontweight='bold', fontsize=10, zorder=4)\n",
    "        ax1.text(6.2, 7 - i*2.5, f'{salidas_oculta[i]:.2f}', ha='center', va='center', \n",
    "                fontsize=9, bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.3))\n",
    "    \n",
    "    # Capa de salida\n",
    "    for i in range(2):\n",
    "        circle = plt.Circle((8.5, 6 - i*3), 0.4, color='#45B7D1', alpha=0.8, zorder=3)\n",
    "        ax1.add_patch(circle)\n",
    "        ax1.text(8.5, 6 - i*3, f'y{i}', ha='center', va='center', fontweight='bold', fontsize=10, zorder=4)\n",
    "        ax1.text(9.7, 6 - i*3, f'{salidas_finales[i]:.2f}', ha='center', va='center', \n",
    "                fontsize=9, bbox=dict(boxstyle='round,pad=0.3', facecolor='lightgreen', alpha=0.5))\n",
    "    \n",
    "    # Etiquetas de capas\n",
    "    ax1.text(1.5, 9, 'Entrada', ha='center', fontsize=12, fontweight='bold')\n",
    "    ax1.text(5, 9, 'Capa Oculta', ha='center', fontsize=12, fontweight='bold')\n",
    "    ax1.text(8.5, 9, 'Salida', ha='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # --- GRÁFICA 2: Salidas de la capa oculta ---\n",
    "    ax2 = fig.add_subplot(gs[1, 0])\n",
    "    colores_oculta = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "    bars_oculta = ax2.bar(range(3), salidas_oculta, color=colores_oculta, alpha=0.7)\n",
    "    ax2.axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    ax2.set_xlabel('Neurona Oculta', fontsize=11)\n",
    "    ax2.set_ylabel('Valor de Salida', fontsize=11)\n",
    "    ax2.set_title('Salidas de la Capa Oculta', fontsize=13, fontweight='bold', pad=15)\n",
    "    ax2.set_xticks(range(3))\n",
    "    ax2.set_xticklabels(['h0', 'h1', 'h2'])\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Calcular límites con margen\n",
    "    max_oculta = np.max(salidas_oculta) if len(salidas_oculta) > 0 else 1\n",
    "    min_oculta = np.min(salidas_oculta) if len(salidas_oculta) > 0 else 0\n",
    "    rango_oculta = max_oculta - min_oculta\n",
    "    margen_oculta = rango_oculta * 0.2 if rango_oculta > 0 else 1\n",
    "    ax2.set_ylim(min_oculta - margen_oculta, max_oculta + margen_oculta)\n",
    "    \n",
    "    # Añadir valores\n",
    "    for i, v in enumerate(salidas_oculta):\n",
    "        offset = rango_oculta * 0.05 if rango_oculta > 0 else 0.1\n",
    "        ax2.text(i, v + offset if v > 0 else v - offset, f'{v:.2f}', \n",
    "                ha='center', va='bottom' if v > 0 else 'top', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    # --- GRÁFICA 3: Salidas finales ---\n",
    "    ax3 = fig.add_subplot(gs[1, 1])\n",
    "    colores_salida = ['#95E1D3', '#F38181']\n",
    "    bars_salida = ax3.bar(range(2), salidas_finales, color=colores_salida, alpha=0.7)\n",
    "    ax3.axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    ax3.set_xlabel('Neurona de Salida', fontsize=11)\n",
    "    ax3.set_ylabel('Valor de Salida', fontsize=11)\n",
    "    ax3.set_title('Salidas Finales de la Red', fontsize=13, fontweight='bold', pad=15)\n",
    "    ax3.set_xticks(range(2))\n",
    "    ax3.set_xticklabels(['y0', 'y1'])\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Calcular límites con margen\n",
    "    max_salida = np.max(salidas_finales) if len(salidas_finales) > 0 else 1\n",
    "    min_salida = np.min(salidas_finales) if len(salidas_finales) > 0 else 0\n",
    "    rango_salida = max_salida - min_salida\n",
    "    margen_salida = rango_salida * 0.2 if rango_salida > 0 else 1\n",
    "    ax3.set_ylim(min_salida - margen_salida, max_salida + margen_salida)\n",
    "    \n",
    "    # Añadir valores\n",
    "    for i, v in enumerate(salidas_finales):\n",
    "        offset = rango_salida * 0.05 if rango_salida > 0 else 0.1\n",
    "        ax3.text(i, v + offset if v > 0 else v - offset, f'{v:.2f}', \n",
    "                ha='center', va='bottom' if v > 0 else 'top', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    # --- GRÁFICA 4: Mapa de calor de pesos capa oculta ---\n",
    "    ax4 = fig.add_subplot(gs[2, 0])\n",
    "    im1 = ax4.imshow(pesos_oculta, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)\n",
    "    ax4.set_xlabel('Entrada', fontsize=11)\n",
    "    ax4.set_ylabel('Neurona Oculta', fontsize=11)\n",
    "    ax4.set_title('Pesos Capa Oculta (Entrada → Oculta)', fontsize=13, fontweight='bold', pad=15)\n",
    "    ax4.set_xticks(range(3))\n",
    "    ax4.set_yticks(range(3))\n",
    "    ax4.set_xticklabels(['x0', 'x1', 'x2'])\n",
    "    ax4.set_yticklabels(['h0', 'h1', 'h2'])\n",
    "    \n",
    "    # Añadir valores en el mapa de calor\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            ax4.text(j, i, f'{pesos_oculta[i, j]:.2f}', ha='center', va='center', \n",
    "                    color='white' if abs(pesos_oculta[i, j]) > 0.5 else 'black', fontsize=9)\n",
    "    \n",
    "    plt.colorbar(im1, ax=ax4, label='Valor del Peso')\n",
    "    \n",
    "    # --- GRÁFICA 5: Mapa de calor de pesos capa salida ---\n",
    "    ax5 = fig.add_subplot(gs[2, 1])\n",
    "    im2 = ax5.imshow(pesos_salida, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)\n",
    "    ax5.set_xlabel('Neurona Oculta', fontsize=11)\n",
    "    ax5.set_ylabel('Neurona Salida', fontsize=11)\n",
    "    ax5.set_title('Pesos Capa Salida (Oculta → Salida)', fontsize=13, fontweight='bold', pad=15)\n",
    "    ax5.set_xticks(range(3))\n",
    "    ax5.set_yticks(range(2))\n",
    "    ax5.set_xticklabels(['h0', 'h1', 'h2'])\n",
    "    ax5.set_yticklabels(['y0', 'y1'])\n",
    "    \n",
    "    # Añadir valores en el mapa de calor\n",
    "    for i in range(2):\n",
    "        for j in range(3):\n",
    "            ax5.text(j, i, f'{pesos_salida[i, j]:.2f}', ha='center', va='center', \n",
    "                    color='white' if abs(pesos_salida[i, j]) > 0.5 else 'black', fontsize=9)\n",
    "    \n",
    "    plt.colorbar(im2, ax=ax5, label='Valor del Peso')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Información numérica detallada\n",
    "    print(\"=\" * 70)\n",
    "    print(\" PROPAGACIÓN HACIA ADELANTE (FORWARD PASS)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\n ENTRADAS: {entradas}\")\n",
    "    print(f\"\\n CAPA OCULTA:\")\n",
    "    for i in range(3):\n",
    "        print(f\"  h{i} = np.dot({entradas}, {pesos_oculta[i]}) + {sesgos_oculta[i]:.2f}\")\n",
    "        print(f\"     = {np.dot(entradas, pesos_oculta[i]):.3f} + {sesgos_oculta[i]:.2f} = {salidas_oculta[i]:.3f}\")\n",
    "    \n",
    "    print(f\"\\n CAPA DE SALIDA:\")\n",
    "    for i in range(2):\n",
    "        print(f\"  y{i} = np.dot({salidas_oculta}, {pesos_salida[i]}) + {sesgos_salida[i]:.2f}\")\n",
    "        print(f\"     = {np.dot(salidas_oculta, pesos_salida[i]):.3f} + {sesgos_salida[i]:.2f} = {salidas_finales[i]:.3f}\")\n",
    "    \n",
    "    print(f\"\\n SALIDAS FINALES: {salidas_finales}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "# Crear sliders\n",
    "slider_oculta = widgets.FloatSlider(\n",
    "    value=1.0, min=-1.0, max=1.0, step=0.05,\n",
    "    description='Mult. Pesos Oculta:',\n",
    "    continuous_update=True,\n",
    "    style={'description_width': '150px'}\n",
    ")\n",
    "\n",
    "slider_salida = widgets.FloatSlider(\n",
    "    value=1.0, min=-1.0, max=1.0, step=0.05,\n",
    "    description='Mult. Pesos Salida:',\n",
    "    continuous_update=True,\n",
    "    style={'description_width': '150px'}\n",
    ")\n",
    "\n",
    "# Crear interfaz interactiva\n",
    "print(\" Ajusta los multiplicadores para ver cómo cambian las salidas de la red neuronal\\n\")\n",
    "widgets.interact(calcular_y_visualizar,\n",
    "                mult_oculta=slider_oculta,\n",
    "                mult_salida=slider_salida)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d20a410",
   "metadata": {},
   "source": [
    "## Lotes, capas y objetos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd22bcb-987c-4762-adf2-11a99022aa3e",
   "metadata": {},
   "source": [
    "Hasta ahora hemos trabajado con un único vector de entrada, es decir, una sola muestra del proceso que queremos modelar.  \n",
    "Sin embargo, en la práctica resulta mucho más eficiente procesar **varias muestras al mismo tiempo**.  \n",
    "A este conjunto de muestras se le denomina **lote** (*batch*).\n",
    "\n",
    "Cada elemento del vector de entrada representa un tipo de dato relevante del sistema que se desea predecir o analizar —por ejemplo, **temperatura**, **voltaje**, **corriente**, **presión**, etc.  \n",
    "Al agrupar muchas de estas mediciones en un lote, podemos aprovechar las ventajas del procesamiento paralelo, **reduciendo el tiempo de cálculo** y **mejorando la estabilidad del entrenamiento** del modelo.  \n",
    "\n",
    "Durante el entrenamiento (tema que se abordará más adelante), el uso de lotes permite **ajustar los pesos de las neuronas de manera más precisa**, ya que la actualización de parámetros se basa en la información combinada de múltiples ejemplos.  \n",
    "Además, mostrar los datos **por lotes pequeños pero representativos** evita que la red memorice los ejemplos (lo que se conoce como *overfitting*) y mejora su capacidad de generalización.\n",
    "\n",
    "Matemáticamente, esto implica trabajar con **matrices** en lugar de vectores:  \n",
    "- Cada **fila** de la matriz de entradas representa una muestra (un conjunto de mediciones).  \n",
    "- Cada **columna** representa una característica o variable (por ejemplo, temperatura, voltaje, corriente...).  \n",
    "- Los **pesos** también se organizan en una matriz, donde cada **columna de pesos** corresponde a las conexiones hacia una neurona de la capa siguiente.\n",
    "\n",
    "Por las reglas de la multiplicación de matrices, es necesario **transponer la matriz de pesos** para que las dimensiones coincidan correctamente:  \n",
    "si la matriz de entradas tiene forma `(n_muestras, n_entradas)` y los pesos `(n_neuronas, n_entradas)`, entonces debemos multiplicar por `pesos.T`.\n",
    "\n",
    "A continuación, se muestra un ejemplo práctico con NumPy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d27be96b-fb4d-47f1-92c5-c3d06f1d8861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salidas de la capa:\n",
      "[[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Supongamos un lote (batch) de 3 muestras, cada una con 4 características\n",
    "entradas = np.array([\n",
    "    [1.0, 2.0, 3.0, 2.5],\n",
    "    [2.0, 5.0, -1.0, 2.0],\n",
    "    [-1.5, 2.7, 3.3, -0.8]\n",
    "])\n",
    "\n",
    "# Definimos una capa con 3 neuronas, cada una con 4 pesos (uno por entrada)\n",
    "pesos = np.array([\n",
    "    [0.2, 0.8, -0.5, 1.0],\n",
    "    [0.5, -0.91, 0.26, -0.5],\n",
    "    [-0.26, -0.27, 0.17, 0.87]\n",
    "])\n",
    "\n",
    "# Bias de cada neurona\n",
    "bias = np.array([2.0, 3.0, 0.5])\n",
    "\n",
    "# Cálculo de la salida de la capa para todo el lote\n",
    "salidas = np.dot(entradas, pesos.T) + bias\n",
    "\n",
    "print(\"Salidas de la capa:\")\n",
    "print(salidas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70fc78a-97b1-48ed-bbdd-2ad7f62eb5dc",
   "metadata": {},
   "source": [
    "Un ejemplo interactivo en el que se puede ejemplificar mejor el concepto de batches o lotes y el por qué de transponer los datos es el siguiente: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f25c9fa1-bd56-42e9-a348-5a720ae27092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ajusta el tamano del batch y numero de neuronas para ver las operaciones matriciales\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb93cb365b24241b5e68eaad7a9e3cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=3, continuous_update=False, description='Num. Muestras:', max=5, min=1, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.visualizar_batches(n_muestras, n_neuronas)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualizar_batches(n_muestras, n_neuronas):\n",
    "    \"\"\"Visualiza el procesamiento de batches con operaciones matriciales\"\"\"\n",
    "    \n",
    "    # Configuración fija\n",
    "    n_entradas = 4\n",
    "    \n",
    "    # Generar datos de ejemplo\n",
    "    np.random.seed(42)\n",
    "    entradas = np.random.randn(n_muestras, n_entradas) * 2\n",
    "    pesos = np.random.randn(n_neuronas, n_entradas)\n",
    "    bias = np.random.randn(n_neuronas)\n",
    "    \n",
    "    # Calcular salidas\n",
    "    salidas = np.dot(entradas, pesos.T) + bias\n",
    "    \n",
    "    # Simular tiempos de procesamiento (valores más realistas)\n",
    "    # Tiempo base por operación (unidades arbitrarias)\n",
    "    tiempo_base_por_muestra = 10.0\n",
    "    \n",
    "    # Tiempo procesando muestra por muestra (lineal con el número de muestras)\n",
    "    tiempo_individual = tiempo_base_por_muestra * n_muestras\n",
    "    \n",
    "    # Tiempo procesando en batch (con overhead fijo + escalado sublineal)\n",
    "    overhead_batch = 3.0  # Overhead inicial de preparar el batch\n",
    "    tiempo_batch = overhead_batch + (tiempo_base_por_muestra * n_muestras * 0.3)\n",
    "    \n",
    "    # Cerrar figuras previas\n",
    "    plt.close('all')\n",
    "    \n",
    "    # Crear visualización\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    gs = fig.add_gridspec(4, 3, hspace=0.45, wspace=0.35)\n",
    "    \n",
    "    # --- GRÁFICA 1: Matriz de Entradas ---\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    im1 = ax1.imshow(entradas, cmap='viridis', aspect='auto')\n",
    "    ax1.set_title(f'Matriz de Entradas\\n({n_muestras} muestras x {n_entradas} caracteristicas)', \n",
    "                  fontsize=11, fontweight='bold', pad=10)\n",
    "    ax1.set_xlabel('Caracteristicas (features)', fontsize=9)\n",
    "    ax1.set_ylabel('Muestras (samples)', fontsize=9)\n",
    "    ax1.set_xticks(range(n_entradas))\n",
    "    ax1.set_yticks(range(n_muestras))\n",
    "    ax1.set_xticklabels([f'x{i}' for i in range(n_entradas)], fontsize=8)\n",
    "    ax1.set_yticklabels([f'M{i}' for i in range(n_muestras)], fontsize=8)\n",
    "    \n",
    "    # Añadir valores en la matriz\n",
    "    for i in range(n_muestras):\n",
    "        for j in range(n_entradas):\n",
    "            color = 'white' if abs(entradas[i, j]) > 1 else 'black'\n",
    "            ax1.text(j, i, f'{entradas[i, j]:.1f}', ha='center', va='center', \n",
    "                    color=color, fontsize=7)\n",
    "    \n",
    "    plt.colorbar(im1, ax=ax1, fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # --- GRÁFICA 2: Matriz de Pesos ---\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    im2 = ax2.imshow(pesos, cmap='coolwarm', aspect='auto', vmin=-2, vmax=2)\n",
    "    ax2.set_title(f'Matriz de Pesos\\n({n_neuronas} neuronas x {n_entradas} pesos)', \n",
    "                  fontsize=11, fontweight='bold', pad=10)\n",
    "    ax2.set_xlabel('Pesos por entrada', fontsize=9)\n",
    "    ax2.set_ylabel('Neuronas', fontsize=9)\n",
    "    ax2.set_xticks(range(n_entradas))\n",
    "    ax2.set_yticks(range(n_neuronas))\n",
    "    ax2.set_xticklabels([f'w{i}' for i in range(n_entradas)], fontsize=8)\n",
    "    ax2.set_yticklabels([f'N{i}' for i in range(n_neuronas)], fontsize=8)\n",
    "    \n",
    "    # Añadir valores en la matriz\n",
    "    for i in range(n_neuronas):\n",
    "        for j in range(n_entradas):\n",
    "            color = 'white' if abs(pesos[i, j]) > 1 else 'black'\n",
    "            ax2.text(j, i, f'{pesos[i, j]:.1f}', ha='center', va='center', \n",
    "                    color=color, fontsize=7)\n",
    "    \n",
    "    plt.colorbar(im2, ax=ax2, fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # --- GRÁFICA 3: Matriz de Pesos Transpuesta ---\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    im3 = ax3.imshow(pesos.T, cmap='coolwarm', aspect='auto', vmin=-2, vmax=2)\n",
    "    ax3.set_title(f'Matriz de Pesos Transpuesta\\n({n_entradas} x {n_neuronas})', \n",
    "                  fontsize=11, fontweight='bold', pad=10)\n",
    "    ax3.set_xlabel('Neuronas', fontsize=9)\n",
    "    ax3.set_ylabel('Pesos', fontsize=9)\n",
    "    ax3.set_xticks(range(n_neuronas))\n",
    "    ax3.set_yticks(range(n_entradas))\n",
    "    ax3.set_xticklabels([f'N{i}' for i in range(n_neuronas)], fontsize=8)\n",
    "    ax3.set_yticklabels([f'w{i}' for i in range(n_entradas)], fontsize=8)\n",
    "    \n",
    "    # Añadir valores en la matriz\n",
    "    for i in range(n_entradas):\n",
    "        for j in range(n_neuronas):\n",
    "            color = 'white' if abs(pesos.T[i, j]) > 1 else 'black'\n",
    "            ax3.text(j, i, f'{pesos.T[i, j]:.1f}', ha='center', va='center', \n",
    "                    color=color, fontsize=7)\n",
    "    \n",
    "    plt.colorbar(im3, ax=ax3, fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # --- GRÁFICA 4: Diagrama de operación ---\n",
    "    ax4 = fig.add_subplot(gs[1, :])\n",
    "    ax4.set_xlim(0, 10)\n",
    "    ax4.set_ylim(0, 3)\n",
    "    ax4.axis('off')\n",
    "    ax4.set_title('Operacion Matricial: Entradas x Pesos.T + Bias', \n",
    "                  fontsize=13, fontweight='bold', pad=15)\n",
    "    \n",
    "    # Dibujar las operaciones\n",
    "    ax4.text(1, 1.5, f'Entradas\\n[{n_muestras}x{n_entradas}]', ha='center', va='center', \n",
    "            fontsize=10, bbox=dict(boxstyle='round', facecolor='#88D498', alpha=0.7))\n",
    "    ax4.text(2.5, 1.5, 'x', ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "    ax4.text(4, 1.5, f'Pesos.T\\n[{n_entradas}x{n_neuronas}]', ha='center', va='center', \n",
    "            fontsize=10, bbox=dict(boxstyle='round', facecolor='#FF6B9D', alpha=0.7))\n",
    "    ax4.text(5.5, 1.5, '+', ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "    ax4.text(7, 1.5, f'Bias\\n[{n_neuronas}]', ha='center', va='center', \n",
    "            fontsize=10, bbox=dict(boxstyle='round', facecolor='#C5A3FF', alpha=0.7))\n",
    "    ax4.text(8.3, 1.5, '=', ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "    ax4.text(9.3, 1.5, f'Salidas\\n[{n_muestras}x{n_neuronas}]', ha='center', va='center', \n",
    "            fontsize=10, bbox=dict(boxstyle='round', facecolor='#FFD93D', alpha=0.7))\n",
    "    \n",
    "    # Explicación\n",
    "    ax4.text(5, 0.3, 'Todas las muestras se procesan en paralelo', \n",
    "            ha='center', fontsize=11, style='italic', color='#2D5016', fontweight='bold')\n",
    "    \n",
    "    # --- GRÁFICA 5: Matriz de Salidas ---\n",
    "    ax5 = fig.add_subplot(gs[2, 0])\n",
    "    im5 = ax5.imshow(salidas, cmap='plasma', aspect='auto')\n",
    "    ax5.set_title(f'Matriz de Salidas\\n({n_muestras} muestras x {n_neuronas} neuronas)', \n",
    "                  fontsize=11, fontweight='bold', pad=10)\n",
    "    ax5.set_xlabel('Neuronas', fontsize=9)\n",
    "    ax5.set_ylabel('Muestras', fontsize=9)\n",
    "    ax5.set_xticks(range(n_neuronas))\n",
    "    ax5.set_yticks(range(n_muestras))\n",
    "    ax5.set_xticklabels([f'N{i}' for i in range(n_neuronas)], fontsize=8)\n",
    "    ax5.set_yticklabels([f'M{i}' for i in range(n_muestras)], fontsize=8)\n",
    "    \n",
    "    # Añadir valores en la matriz\n",
    "    for i in range(n_muestras):\n",
    "        for j in range(n_neuronas):\n",
    "            color = 'white' if abs(salidas[i, j]) > 2 else 'black'\n",
    "            ax5.text(j, i, f'{salidas[i, j]:.1f}', ha='center', va='center', \n",
    "                    color=color, fontsize=7)\n",
    "    \n",
    "    plt.colorbar(im5, ax=ax5, fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # --- GRÁFICA 6: Salidas por muestra ---\n",
    "    ax6 = fig.add_subplot(gs[2, 1:])\n",
    "    x = np.arange(n_neuronas)\n",
    "    width = 0.8 / n_muestras\n",
    "    colores = plt.cm.Set3(np.linspace(0, 1, n_muestras))\n",
    "    \n",
    "    for i in range(n_muestras):\n",
    "        offset = (i - n_muestras/2) * width + width/2\n",
    "        ax6.bar(x + offset, salidas[i], width, label=f'Muestra {i}', \n",
    "               color=colores[i], alpha=0.8)\n",
    "    \n",
    "    ax6.set_xlabel('Neurona', fontsize=10)\n",
    "    ax6.set_ylabel('Valor de Salida', fontsize=10)\n",
    "    ax6.set_title('Salidas de cada Muestra por Neurona', fontsize=11, fontweight='bold', pad=10)\n",
    "    ax6.set_xticks(x)\n",
    "    ax6.set_xticklabels([f'N{i}' for i in range(n_neuronas)])\n",
    "    ax6.legend(fontsize=8, loc='best')\n",
    "    ax6.grid(axis='y', alpha=0.3)\n",
    "    ax6.axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    \n",
    "    # --- GRÁFICA 7: Comparación de eficiencia ---\n",
    "    ax7 = fig.add_subplot(gs[3, :])\n",
    "    \n",
    "    metodos = ['Muestra por Muestra\\n(Secuencial)', 'Procesamiento en Batch\\n(Paralelo)']\n",
    "    tiempos = [tiempo_individual, tiempo_batch]\n",
    "    colores_barras = ['#FF6B6B', '#4ECDC4']\n",
    "    \n",
    "    y_pos = np.arange(len(metodos))\n",
    "    bars = ax7.barh(y_pos, tiempos, color=colores_barras, alpha=0.7, height=0.5)\n",
    "    \n",
    "    ax7.set_yticks(y_pos)\n",
    "    ax7.set_yticklabels(metodos, fontsize=10)\n",
    "    ax7.set_xlabel('Tiempo de Procesamiento (unidades arbitrarias)', fontsize=10)\n",
    "    ax7.set_title('Comparacion de Eficiencia: Secuencial vs Batch', \n",
    "                  fontsize=12, fontweight='bold', pad=15)\n",
    "    ax7.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Establecer límites apropiados\n",
    "    max_tiempo = max(tiempos)\n",
    "    ax7.set_xlim([0, max_tiempo * 1.25])\n",
    "    \n",
    "    # Añadir valores en las barras\n",
    "    for i, (bar, tiempo) in enumerate(zip(bars, tiempos)):\n",
    "        ax7.text(tiempo + max_tiempo * 0.02, bar.get_y() + bar.get_height()/2, \n",
    "                f'{tiempo:.1f}', va='center', ha='left', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    # Calcular y mostrar speedup\n",
    "    speedup = tiempo_individual / tiempo_batch if tiempo_batch > 0 else 1\n",
    "    \n",
    "    # Texto del speedup con explicación según el caso\n",
    "    if n_muestras == 1:\n",
    "        speedup_text = f'Con 1 muestra: Batch tiene overhead inicial (~{overhead_batch:.0f} unidades)\\nPero escala mejor con mas muestras!'\n",
    "    else:\n",
    "        speedup_text = f'Speedup: {speedup:.1f}x mas rapido con batches!\\nProcesar {n_muestras} muestras en paralelo es mucho mas eficiente'\n",
    "    \n",
    "    ax7.text(max_tiempo * 0.5, -0.85, speedup_text,\n",
    "            ha='center', va='top', fontsize=10, fontweight='bold', \n",
    "            bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Información numérica\n",
    "    print(\"=\" * 80)\n",
    "    print(\"RESUMEN DE OPERACIONES MATRICIALES CON BATCHES\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nDIMENSIONES:\")\n",
    "    print(f\"   Entradas:     {entradas.shape} = ({n_muestras} muestras, {n_entradas} caracteristicas)\")\n",
    "    print(f\"   Pesos:        {pesos.shape} = ({n_neuronas} neuronas, {n_entradas} pesos c/u)\")\n",
    "    print(f\"   Pesos.T:      {pesos.T.shape} = ({n_entradas} caracteristicas, {n_neuronas} neuronas)\")\n",
    "    print(f\"   Bias:         {bias.shape} = ({n_neuronas} valores)\")\n",
    "    print(f\"   Salidas:      {salidas.shape} = ({n_muestras} muestras, {n_neuronas} neuronas)\")\n",
    "    \n",
    "    print(f\"\\nOPERACION:\")\n",
    "    print(f\"   salidas = np.dot(entradas, pesos.T) + bias\")\n",
    "    print(f\"   salidas = np.dot({entradas.shape}, {pesos.T.shape}) + {bias.shape}\")\n",
    "    print(f\"   salidas = {salidas.shape}\")\n",
    "    \n",
    "    print(f\"\\nEFICIENCIA:\")\n",
    "    print(f\"   Tiempo secuencial (muestra por muestra): {tiempo_individual:.1f} unidades\")\n",
    "    print(f\"   Tiempo batch (procesamiento paralelo):   {tiempo_batch:.1f} unidades\")\n",
    "    print(f\"   Speedup:                                   {speedup:.2f}x\")\n",
    "    \n",
    "    if n_muestras == 1:\n",
    "        print(f\"\\n   NOTA: Con 1 muestra, el batch tiene overhead inicial.\")\n",
    "        print(f\"         La ventaja aparece con multiples muestras (prueba con 3-5 muestras)!\")\n",
    "    \n",
    "    print(f\"\\nINTERPRETACION:\")\n",
    "    print(f\"   - Cada FILA de 'entradas' = 1 muestra con {n_entradas} caracteristicas\")\n",
    "    print(f\"   - Cada COLUMNA de 'entradas' = 1 caracteristica medida en {n_muestras} muestras\")\n",
    "    print(f\"   - Cada FILA de 'salidas' = salida de las {n_neuronas} neuronas para 1 muestra\")\n",
    "    print(f\"   - Se procesan {n_muestras} muestras simultaneamente (paraleizacion)\")\n",
    "    \n",
    "    print(f\"\\nEJEMPLO DE SALIDA (Muestra 0):\")\n",
    "    print(f\"   Entrada: {entradas[0]}\")\n",
    "    print(f\"   Salida:  {salidas[0]}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Crear sliders\n",
    "slider_muestras = widgets.IntSlider(\n",
    "    value=3, min=1, max=5, step=1,\n",
    "    description='Num. Muestras:',\n",
    "    continuous_update=False,\n",
    "    style={'description_width': '120px'}\n",
    ")\n",
    "\n",
    "slider_neuronas = widgets.IntSlider(\n",
    "    value=3, min=2, max=5, step=1,\n",
    "    description='Num. Neuronas:',\n",
    "    continuous_update=False,\n",
    "    style={'description_width': '120px'}\n",
    ")\n",
    "\n",
    "# Crear interfaz interactiva\n",
    "print(\"Ajusta el tamano del batch y numero de neuronas para ver las operaciones matriciales\\n\")\n",
    "widgets.interact(visualizar_batches,\n",
    "                n_muestras=slider_muestras,\n",
    "                n_neuronas=slider_neuronas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163d1ff5-6b4d-4d55-83c9-55f341fe88e4",
   "metadata": {},
   "source": [
    "- Programación Orientada a Objetos (POO) para Redes Neuronales\n",
    "\n",
    "Hasta este punto, hemos implementado el cálculo de la salida de una capa de forma manual.  \n",
    "Sin embargo, a medida que se agregan más capas o se modifican las dimensiones de entrada y salida, el código puede volverse repetitivo y difícil de mantener.  \n",
    "\n",
    "Para resolver este problema, es útil aplicar los principios de la **Programación Orientada a Objetos (POO)**.  \n",
    "Con la POO podemos definir una **clase `Layer_Dense`** que represente una capa densa (totalmente conectada) y que almacene tanto sus **pesos**, **biases** como el método que calcula su **propagación hacia adelante** (*forward pass*).  \n",
    "\n",
    "De este modo, podemos crear fácilmente varias capas con diferentes tamaños y conectarlas entre sí, sin tener que reescribir el mismo código cada vez.  \n",
    "Esto permite construir redes neuronales más profundas simplemente **instanciando nuevas capas** y pasándoles la salida de la capa anterior como entrada.\n",
    "\n",
    "A continuación, se muestra el código completo con NumPy, implementando dos capas densas conectadas secuencialmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ef40dff-fb05-4a0b-aced-9d1e6aeb361d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salida de la primera capa:\n",
      "[[ 0.10758131  1.03983522  0.24462411  0.31821498  0.18851053]\n",
      " [-0.08349796  0.70846411  0.00293357  0.44701525  0.36360538]\n",
      " [-0.50763245  0.55688422  0.07987797 -0.34889573  0.04553042]]\n",
      "\n",
      "Salida de la segunda capa:\n",
      "[[ 0.148296   -0.08397602]\n",
      " [ 0.14100315 -0.01340469]\n",
      " [ 0.20124979 -0.07290616]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Fijamos la semilla aleatoria para reproducibilidad\n",
    "np.random.seed(0)\n",
    "\n",
    "# Definimos una matriz de entrada X (3 muestras, 4 características cada una)\n",
    "X = [\n",
    "    [1.0, 2.0, 3.0, 2.5],\n",
    "    [2.0, 5.0, -1.0, 2.0],\n",
    "    [-1.5, 2.7, 3.3, -0.8]\n",
    "]\n",
    "\n",
    "# Definición de la clase Layer_Dense\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Inicializamos los pesos con valores aleatorios pequeños\n",
    "        # np.random.randn(n_inputs, n_neurons) genera una matriz de dimensiones\n",
    "        # [n_inputs x n_neurons], donde cada columna corresponde a los pesos de una neurona.\n",
    "        # Por eso ya NO es necesario usar la transpuesta al multiplicar.\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "        \n",
    "        # Inicializamos los biases en ceros (uno por neurona)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Calcula la salida de la capa como: entradas * pesos + bias\n",
    "        # np.dot realiza la multiplicación matricial entre inputs y weights\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# Creamos dos capas densas:\n",
    "# - La primera recibe 4 entradas y tiene 5 neuronas.\n",
    "# - La segunda recibe 5 entradas (las salidas de la anterior) y produce 2 salidas.\n",
    "layer1 = Layer_Dense(4, 5)\n",
    "layer2 = Layer_Dense(5, 2)\n",
    "\n",
    "# Pasamos la entrada X por la primera capa\n",
    "layer1.forward(X)\n",
    "\n",
    "# Pasamos la salida de la primera capa como entrada a la segunda\n",
    "layer2.forward(layer1.output)\n",
    "\n",
    "# Imprimimos los resultados\n",
    "print(\"Salida de la primera capa:\")\n",
    "print(layer1.output)\n",
    "\n",
    "print(\"\\nSalida de la segunda capa:\")\n",
    "print(layer2.output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c509b5",
   "metadata": {},
   "source": [
    "## Funciones de activacion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5365cd-59ad-4296-ae96-772f389e5ae3",
   "metadata": {},
   "source": [
    "En una red neuronal, **la función de activación** es la operación que se aplica a la salida de cada neurona **después del cálculo lineal** (es decir, después del producto entre las entradas y los pesos más el bias).  \n",
    "\n",
    "Su propósito es **introducir no linealidad** en el modelo, permitiendo que la red aprenda relaciones complejas entre las variables de entrada y salida.  \n",
    "\n",
    "Si solo utilizáramos operaciones lineales (multiplicaciones y sumas), sin una función de activación, toda la red se comportaría como una **única transformación lineal**, sin importar cuántas capas tuviera.  \n",
    "\n",
    "En otras palabras, sin funciones de activación, la red **no podría aproximar funciones no lineales** ni capturar patrones complejos en los datos.\n",
    "\n",
    "De forma intuitiva, las funciones de activación pueden verse como un mecanismo que **decide qué neuronas \"se disparan\" o permanecen inactivas**, similar al comportamiento de las neuronas biológicas del cerebro.\n",
    "\n",
    "### Tipos comunes de funciones de activación\n",
    "\n",
    "1. **Función Step (Escalón)**  \n",
    "   Es la más simple. Devuelve 1 si la entrada es positiva, y 0 en caso contrario.  \n",
    "   Se usó en los primeros modelos neuronales (como el perceptrón), pero casi no se utiliza hoy en día debido a que no permite un ajuste gradual ni continuidad en la salida.\n",
    "   \n",
    "   $$\n",
    "   f(x) =\n",
    "   \\begin{cases}\n",
    "   1, & \\text{si } x \\ge 0 \\\\\n",
    "   0, & \\text{si } x < 0\n",
    "   \\end{cases}\n",
    "   $$\n",
    "\n",
    "2. **Función Sigmoide**  \n",
    "   Convierte cualquier valor real en un número entre 0 y 1, con una forma suave y continua.  \n",
    "   Se usa principalmente en capas de salida cuando se requiere interpretar la salida como una probabilidad.\n",
    "   \n",
    "   $$\n",
    "   f(x) = \\frac{1}{1 + e^{-x}}\n",
    "   $$\n",
    "\n",
    "3. **Función ReLU (*Rectified Linear Unit*)**  \n",
    "   Es la función más popular para las **capas ocultas** debido a su sencillez y efectividad.  \n",
    "   Devuelve el valor de entrada si es positivo y 0 si es negativo, permitiendo que el modelo mantenga una parte de la información (granularidad) mientras descarta valores negativos.\n",
    "   \n",
    "   $$\n",
    "   f(x) = \\max(0, x)\n",
    "   $$\n",
    "\n",
    "En general, distintas capas pueden tener **distintas funciones de activación** dependiendo de la tarea.  \n",
    "Por ejemplo, las capas ocultas suelen usar ReLU, mientras que las capas de salida pueden usar *softmax* o *sigmoid* según el tipo de problema (clasificación o regresión).  \n",
    "Este tema se tratará en mayor profundidad más adelante.\n",
    "\n",
    "A continuación, se muestra un ejemplo práctico en Python implementando la función ReLU sobre una capa neuronal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcb4945b-dbcf-4085-9385-f8b8b31e0ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salida lineal de la capa:\n",
      "[[ 0.10758131  1.03983522  0.24462411  0.31821498  0.18851053]\n",
      " [-0.08349796  0.70846411  0.00293357  0.44701525  0.36360538]\n",
      " [-0.50763245  0.55688422  0.07987797 -0.34889573  0.04553042]]\n",
      "\n",
      "Salida después de aplicar ReLU:\n",
      "[[0.10758131 1.03983522 0.24462411 0.31821498 0.18851053]\n",
      " [0.         0.70846411 0.00293357 0.44701525 0.36360538]\n",
      " [0.         0.55688422 0.07987797 0.         0.04553042]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Fijamos la semilla para reproducibilidad\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Entradas: 3 muestras con 4 características cada una\n",
    "\n",
    "X = [\n",
    "\n",
    "    [1.0, 2.0, 3.0, 2.5],\n",
    "\n",
    "    [2.0, 5.0, -1.0, 2.0],\n",
    "\n",
    "    [-1.5, 2.7, 3.3, -0.8]\n",
    "\n",
    "]\n",
    "\n",
    "# Definición de una capa densa (como antes)\n",
    "\n",
    "class Layer_Dense:\n",
    "\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# Definición de la función ReLU\n",
    "\n",
    "class Activation_ReLU:\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # np.maximum aplica ReLU elemento a elemento\n",
    "\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "# Creamos una capa densa de 4 entradas y 5 neuronas\n",
    "\n",
    "layer1 = Layer_Dense(4, 5)\n",
    "\n",
    "# Calculamos su salida\n",
    "\n",
    "layer1.forward(X)\n",
    "\n",
    "# Aplicamos la activación ReLU\n",
    "\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "activation1.forward(layer1.output)\n",
    "\n",
    "# Mostramos resultados\n",
    "\n",
    "print(\"Salida lineal de la capa:\")\n",
    "\n",
    "print(layer1.output)\n",
    "\n",
    "print(\"\\nSalida después de aplicar ReLU:\")\n",
    "\n",
    "print(activation1.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66edc81a-3137-4836-b436-3db15a13987c",
   "metadata": {},
   "source": [
    "Un ejemplo más interactivo es el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2571feb-7945-4ace-9d0d-bb3eedd5952d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ajusta los parametros para ver como las funciones de activacion transforman las salidas\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a2d52363dc491987b66c0470c3ea99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, continuous_update=False, description='Mult. Pesos:', max=3.0, min…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.visualizar_activaciones(mult_pesos, seed)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Definir las funciones de activación\n",
    "def step_function(x):\n",
    "    \"\"\"Función Step (Escalón)\"\"\"\n",
    "    return np.where(x >= 0, 1, 0)\n",
    "\n",
    "def sigmoid_function(x):\n",
    "    \"\"\"Función Sigmoide\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu_function(x):\n",
    "    \"\"\"Función ReLU\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Clases del ejemplo original\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons, seed=0):\n",
    "        np.random.seed(seed)\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "def visualizar_activaciones(mult_pesos, seed):\n",
    "    \"\"\"Visualiza el efecto de diferentes funciones de activación\"\"\"\n",
    "    \n",
    "    # Datos de entrada (3 muestras, 4 características)\n",
    "    X = np.array([\n",
    "        [1.0, 2.0, 3.0, 2.5],\n",
    "        [2.0, 5.0, -1.0, 2.0],\n",
    "        [-1.5, 2.7, 3.3, -0.8]\n",
    "    ])\n",
    "    \n",
    "    # Crear capa densa\n",
    "    layer1 = Layer_Dense(4, 5, seed=seed)\n",
    "    layer1.weights = layer1.weights * mult_pesos  # Aplicar multiplicador\n",
    "    layer1.forward(X)\n",
    "    \n",
    "    # Salida lineal (sin activación)\n",
    "    salida_lineal = layer1.output\n",
    "    \n",
    "    # Aplicar diferentes funciones de activación\n",
    "    salida_step = step_function(salida_lineal)\n",
    "    salida_sigmoid = sigmoid_function(salida_lineal)\n",
    "    salida_relu = relu_function(salida_lineal)\n",
    "    \n",
    "    # Crear figura\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    gs = fig.add_gridspec(3, 3, hspace=0.4, wspace=0.3)\n",
    "    \n",
    "    # --- FILA 1: Gráficas de las funciones de activación ---\n",
    "    x_range = np.linspace(-5, 5, 1000)\n",
    "    \n",
    "    # Step\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.plot(x_range, step_function(x_range), linewidth=2.5, color='#FF6B6B')\n",
    "    ax1.axhline(y=0, color='black', linestyle='--', linewidth=0.8, alpha=0.3)\n",
    "    ax1.axvline(x=0, color='black', linestyle='--', linewidth=0.8, alpha=0.3)\n",
    "    ax1.grid(alpha=0.3)\n",
    "    ax1.set_xlabel('x', fontsize=10)\n",
    "    ax1.set_ylabel('f(x)', fontsize=10)\n",
    "    ax1.set_title('Funcion Step (Escalon)', fontsize=11, fontweight='bold', pad=10)\n",
    "    ax1.set_ylim(-0.2, 1.2)\n",
    "    ax1.text(2.5, 0.5, 'f(x) = 1 si x >= 0\\nf(x) = 0 si x < 0', \n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5), fontsize=8)\n",
    "    \n",
    "    # Sigmoid\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.plot(x_range, sigmoid_function(x_range), linewidth=2.5, color='#4ECDC4')\n",
    "    ax2.axhline(y=0, color='black', linestyle='--', linewidth=0.8, alpha=0.3)\n",
    "    ax2.axvline(x=0, color='black', linestyle='--', linewidth=0.8, alpha=0.3)\n",
    "    ax2.axhline(y=0.5, color='gray', linestyle=':', linewidth=1, alpha=0.5)\n",
    "    ax2.grid(alpha=0.3)\n",
    "    ax2.set_xlabel('x', fontsize=10)\n",
    "    ax2.set_ylabel('f(x)', fontsize=10)\n",
    "    ax2.set_title('Funcion Sigmoide', fontsize=11, fontweight='bold', pad=10)\n",
    "    ax2.set_ylim(-0.2, 1.2)\n",
    "    ax2.text(2.5, 0.5, 'f(x) = 1 / (1 + e^(-x))', \n",
    "            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5), fontsize=8)\n",
    "    \n",
    "    # ReLU\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    ax3.plot(x_range, relu_function(x_range), linewidth=2.5, color='#45B7D1')\n",
    "    ax3.axhline(y=0, color='black', linestyle='--', linewidth=0.8, alpha=0.3)\n",
    "    ax3.axvline(x=0, color='black', linestyle='--', linewidth=0.8, alpha=0.3)\n",
    "    ax3.grid(alpha=0.3)\n",
    "    ax3.set_xlabel('x', fontsize=10)\n",
    "    ax3.set_ylabel('f(x)', fontsize=10)\n",
    "    ax3.set_title('Funcion ReLU', fontsize=11, fontweight='bold', pad=10)\n",
    "    ax3.text(2.5, 2.5, 'f(x) = max(0, x)', \n",
    "            bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5), fontsize=8)\n",
    "    \n",
    "    # --- FILA 2: Salida lineal y comparación ---\n",
    "    \n",
    "    # Salida lineal (antes de activación)\n",
    "    ax4 = fig.add_subplot(gs[1, :])\n",
    "    n_neuronas = salida_lineal.shape[1]\n",
    "    n_muestras = salida_lineal.shape[0]\n",
    "    x_pos = np.arange(n_neuronas)\n",
    "    width = 0.25\n",
    "    \n",
    "    for i in range(n_muestras):\n",
    "        offset = (i - 1) * width\n",
    "        ax4.bar(x_pos + offset, salida_lineal[i], width, \n",
    "               label=f'Muestra {i}', alpha=0.8)\n",
    "    \n",
    "    ax4.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "    ax4.set_xlabel('Neurona', fontsize=10)\n",
    "    ax4.set_ylabel('Valor de salida', fontsize=10)\n",
    "    ax4.set_title('Salida Lineal (Antes de Activacion)', fontsize=12, fontweight='bold', pad=10)\n",
    "    ax4.set_xticks(x_pos)\n",
    "    ax4.set_xticklabels([f'N{i}' for i in range(n_neuronas)])\n",
    "    ax4.legend(fontsize=8, loc='best')\n",
    "    ax4.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # --- FILA 3: Comparación de salidas con diferentes activaciones ---\n",
    "    \n",
    "    # Step\n",
    "    ax5 = fig.add_subplot(gs[2, 0])\n",
    "    im5 = ax5.imshow(salida_step, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "    ax5.set_title('Salida con Step', fontsize=11, fontweight='bold', pad=10)\n",
    "    ax5.set_xlabel('Neurona', fontsize=9)\n",
    "    ax5.set_ylabel('Muestra', fontsize=9)\n",
    "    ax5.set_xticks(range(n_neuronas))\n",
    "    ax5.set_yticks(range(n_muestras))\n",
    "    ax5.set_xticklabels([f'N{i}' for i in range(n_neuronas)], fontsize=8)\n",
    "    ax5.set_yticklabels([f'M{i}' for i in range(n_muestras)], fontsize=8)\n",
    "    \n",
    "    for i in range(n_muestras):\n",
    "        for j in range(n_neuronas):\n",
    "            ax5.text(j, i, f'{salida_step[i, j]:.2f}', ha='center', va='center', \n",
    "                    color='black', fontsize=8, fontweight='bold')\n",
    "    \n",
    "    plt.colorbar(im5, ax=ax5, fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Sigmoid\n",
    "    ax6 = fig.add_subplot(gs[2, 1])\n",
    "    im6 = ax6.imshow(salida_sigmoid, cmap='viridis', aspect='auto', vmin=0, vmax=1)\n",
    "    ax6.set_title('Salida con Sigmoid', fontsize=11, fontweight='bold', pad=10)\n",
    "    ax6.set_xlabel('Neurona', fontsize=9)\n",
    "    ax6.set_ylabel('Muestra', fontsize=9)\n",
    "    ax6.set_xticks(range(n_neuronas))\n",
    "    ax6.set_yticks(range(n_muestras))\n",
    "    ax6.set_xticklabels([f'N{i}' for i in range(n_neuronas)], fontsize=8)\n",
    "    ax6.set_yticklabels([f'M{i}' for i in range(n_muestras)], fontsize=8)\n",
    "    \n",
    "    for i in range(n_muestras):\n",
    "        for j in range(n_neuronas):\n",
    "            color = 'white' if salida_sigmoid[i, j] > 0.5 else 'black'\n",
    "            ax6.text(j, i, f'{salida_sigmoid[i, j]:.2f}', ha='center', va='center', \n",
    "                    color=color, fontsize=8, fontweight='bold')\n",
    "    \n",
    "    plt.colorbar(im6, ax=ax6, fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # ReLU\n",
    "    ax7 = fig.add_subplot(gs[2, 2])\n",
    "    im7 = ax7.imshow(salida_relu, cmap='plasma', aspect='auto')\n",
    "    ax7.set_title('Salida con ReLU', fontsize=11, fontweight='bold', pad=10)\n",
    "    ax7.set_xlabel('Neurona', fontsize=9)\n",
    "    ax7.set_ylabel('Muestra', fontsize=9)\n",
    "    ax7.set_xticks(range(n_neuronas))\n",
    "    ax7.set_yticks(range(n_muestras))\n",
    "    ax7.set_xticklabels([f'N{i}' for i in range(n_neuronas)], fontsize=8)\n",
    "    ax7.set_yticklabels([f'M{i}' for i in range(n_muestras)], fontsize=8)\n",
    "    \n",
    "    for i in range(n_muestras):\n",
    "        for j in range(n_neuronas):\n",
    "            color = 'white' if salida_relu[i, j] > 0.5 else 'black'\n",
    "            ax7.text(j, i, f'{salida_relu[i, j]:.2f}', ha='center', va='center', \n",
    "                    color=color, fontsize=8, fontweight='bold')\n",
    "    \n",
    "    plt.colorbar(im7, ax=ax7, fraction=0.046, pad=0.04)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Información detallada\n",
    "    print(\"=\" * 80)\n",
    "    print(\"COMPARACION DE FUNCIONES DE ACTIVACION\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nSALIDA LINEAL (sin activacion):\")\n",
    "    print(salida_lineal)\n",
    "    print(f\"\\nDespues de STEP:\")\n",
    "    print(salida_step)\n",
    "    print(f\"  - Valores binarios: {np.unique(salida_step)}\")\n",
    "    print(f\"  - Neuronas activas: {np.sum(salida_step)} de {salida_step.size}\")\n",
    "    print(f\"\\nDespues de SIGMOID:\")\n",
    "    print(salida_sigmoid)\n",
    "    print(f\"  - Rango: [{np.min(salida_sigmoid):.3f}, {np.max(salida_sigmoid):.3f}]\")\n",
    "    print(f\"  - Promedio: {np.mean(salida_sigmoid):.3f}\")\n",
    "    print(f\"\\nDespues de RELU:\")\n",
    "    print(salida_relu)\n",
    "    print(f\"  - Valores negativos convertidos a 0\")\n",
    "    print(f\"  - Neuronas muertas (salida=0): {np.sum(salida_relu == 0)} de {salida_relu.size}\")\n",
    "    print(f\"  - Rango activo: [0, {np.max(salida_relu):.3f}]\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CARACTERISTICAS PRINCIPALES:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"STEP:    - Binaria (0 o 1)\")\n",
    "    print(\"         - No diferenciable en x=0\")\n",
    "    print(\"         - Perdida de informacion granular\")\n",
    "    print(\"\\nSIGMOID: - Suave y continua\")\n",
    "    print(\"         - Salida entre 0 y 1\")\n",
    "    print(\"         - Util para probabilidades\")\n",
    "    print(\"         - Problema: vanishing gradient\")\n",
    "    print(\"\\nRELU:    - Simple y eficiente\")\n",
    "    print(\"         - Mantiene valores positivos intactos\")\n",
    "    print(\"         - Elimina valores negativos\")\n",
    "    print(\"         - Mas usado en capas ocultas\")\n",
    "    print(\"         - Problema: neuronas muertas\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Crear sliders\n",
    "slider_pesos = widgets.FloatSlider(\n",
    "    value=1.0, min=0.1, max=3.0, step=0.1,\n",
    "    description='Mult. Pesos:',\n",
    "    continuous_update=False,\n",
    "    style={'description_width': '100px'}\n",
    ")\n",
    "\n",
    "slider_seed = widgets.IntSlider(\n",
    "    value=0, min=0, max=10, step=1,\n",
    "    description='Semilla:',\n",
    "    continuous_update=False,\n",
    "    style={'description_width': '100px'}\n",
    ")\n",
    "\n",
    "# Crear interfaz interactiva\n",
    "print(\"Ajusta los parametros para ver como las funciones de activacion transforman las salidas\\n\")\n",
    "widgets.interact(visualizar_activaciones,\n",
    "                mult_pesos=slider_pesos,\n",
    "                seed=slider_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab155e9f",
   "metadata": {},
   "source": [
    "## Activacion softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f86d227-06e1-4e1a-98b1-e46eb3cc35fa",
   "metadata": {},
   "source": [
    "## Función de Activación Softmax\n",
    "\n",
    "Hasta ahora hemos visto funciones como la **ReLU**, que ayudan a las *hidden layers* a introducir no linealidad en los cálculos.  \n",
    "\n",
    "Sin embargo, cuando llegamos a la **última capa** (la capa de salida), muchas veces queremos **interpretar los resultados de las neuronas como probabilidades**.  \n",
    "\n",
    "Por ejemplo, si una red debe clasificar una entrada en tres clases (gato, perro, pez), no queremos simplemente tres números sin escala, sino algo que nos diga *qué tan probable* es que la entrada pertenezca a cada clase.  \n",
    "\n",
    "Ahí entra **Softmax**.\n",
    "\n",
    "\n",
    "### ¿Qué hace Softmax?\n",
    "\n",
    "Softmax toma los valores de salida de las neuronas (a menudo llamados *logits*) y los convierte en **valores entre 0 y 1**, que además **suman exactamente 1**.  \n",
    "\n",
    "Eso significa que podemos interpretarlos como **probabilidades normalizadas**.\n",
    "\n",
    "Matemáticamente:\n",
    "\n",
    "$$\n",
    "\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### ¿Por qué se usa en la salida?\n",
    "\n",
    "- Porque **permite comparar las salidas de las neuronas** en una misma escala.  \n",
    "- Facilita el cálculo del **error** (que veremos más adelante con *cross-entropy*).  \n",
    "- En clasificación, cada neurona puede representar una clase distinta, y Softmax nos da la probabilidad asociada a cada una.\n",
    "\n",
    "\n",
    "\n",
    "### Prevención de overflow\n",
    "\n",
    "El término $e^{z_i}$ puede crecer muy rápido si $z_i$ es grande.  \n",
    "\n",
    "Para evitar desbordamientos numéricos, restamos el máximo valor de $z$ antes de aplicar la exponencial (esto **no cambia el resultado**, solo lo estabiliza):\n",
    "\n",
    "$$\n",
    "\\text{Softmax}(z_i) = \\frac{e^{z_i - \\max(z)}}{\\sum_{j} e^{z_j - \\max(z)}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### En resumen\n",
    "\n",
    "Softmax:\n",
    "- Normaliza las salidas.  \n",
    "- Permite interpretarlas como probabilidades.  \n",
    "- Se usa casi siempre en la **última capa de redes de clasificación**.  \n",
    "\n",
    "\n",
    "\n",
    "## Ejemplo en Python\n",
    "\n",
    "A continuación implementamos Softmax como una clase de activación, integrada al esquema que venimos usando con las capas densas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c8154b0-d4a0-4680-8dab-ff6f72d6eaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salidas después de Softmax:\n",
      "[[0.30326886 0.40108669 0.29564444]\n",
      " [0.32834771 0.36829707 0.30335522]\n",
      " [0.31750561 0.37292873 0.30956567]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Datos de entrada (3 muestras, 4 características cada una)\n",
    "X = [[1.0, 2.0, 3.0, 2.5],\n",
    "     [2.0, 5.0, -1.0, 2.0],\n",
    "     [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "# Fijar semilla para reproducibilidad\n",
    "np.random.seed(0)\n",
    "\n",
    "# Capa densa general\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Pesos aleatorios pequeños para estabilidad numérica\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "        # Biases inicializados en cero\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Producto punto entre entradas y pesos + biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# Función de activación ReLU\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        # Aplica ReLU: deja pasar positivos, corta negativos\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "# Función de activación Softmax\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        # Restar el máximo para evitar overflow numérico\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # Normalizar dividiendo por la suma de exponentiales (por fila)\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "# Capa oculta (4 entradas → 5 neuronas)\n",
    "layer1 = Layer_Dense(4, 5)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Capa de salida (5 entradas → 3 neuronas)\n",
    "layer2 = Layer_Dense(5, 3)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Paso hacia adelante\n",
    "layer1.forward(X)\n",
    "activation1.forward(layer1.output)\n",
    "\n",
    "layer2.forward(activation1.output)\n",
    "activation2.forward(layer2.output)\n",
    "\n",
    "# Mostrar las probabilidades resultantes\n",
    "print(\"Salidas después de Softmax:\")\n",
    "print(activation2.output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c644ce9-2793-404d-b612-d651d4d2e07b",
   "metadata": {},
   "source": [
    "\n",
    "Todas las filas suman 1, lo que significa que Softmax normalizó correctamente las activaciones.\n",
    "\n",
    "A continuación se presenta un ejemplo interactivo acerca de la implementación de Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6862e77-0b32-49c2-8023-93962a58a6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VISUALIZACION INTERACTIVA DE SOFTMAX\n",
      "================================================================================\n",
      "Ajusta los parametros para explorar como Softmax transforma logits en probabilidades:\n",
      "\n",
      "* Temperatura: Controla que tan 'suave' o 'picuda' es la distribucion\n",
      "* Magnitud Logits: Cambia la escala de los valores de entrada\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85868472b074337832fc67b332f7b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, continuous_update=False, description='Temperatura:', max=5.0, min…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.visualizar_softmax(temperatura, magnitud_logits)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "\n",
    "def visualizar_softmax(temperatura, magnitud_logits):\n",
    "    \"\"\"Visualiza el proceso de Softmax paso a paso\"\"\"\n",
    "    \n",
    "    # Generar logits de ejemplo (3 clases)\n",
    "    np.random.seed(42)\n",
    "    logits_base = np.array([2.0, 1.0, 0.1])\n",
    "    logits = logits_base * magnitud_logits\n",
    "    \n",
    "    # Aplicar Softmax con temperatura\n",
    "    def softmax_con_temp(z, T=1.0):\n",
    "        z_scaled = z / T\n",
    "        # Prevención de overflow\n",
    "        exp_values = np.exp(z_scaled - np.max(z_scaled))\n",
    "        return exp_values / np.sum(exp_values)\n",
    "    \n",
    "    probabilidades = softmax_con_temp(logits, temperatura)\n",
    "    \n",
    "    # Calcular valores intermedios para visualización\n",
    "    logits_shifted = logits - np.max(logits)\n",
    "    exp_values = np.exp(logits_shifted / temperatura)\n",
    "    suma_exp = np.sum(exp_values)\n",
    "    \n",
    "    # Crear figura\n",
    "    fig = plt.figure(figsize=(18, 13))\n",
    "    gs = fig.add_gridspec(4, 3, hspace=0.6, wspace=0.4, top=0.96, bottom=0.04)\n",
    "    \n",
    "    clases = ['Gato', 'Perro', 'Pez']\n",
    "    colores = ['#FF6B6B', '#4ECDC4', '#95E1D3']\n",
    "    \n",
    "    # --- GRAFICA 1: Logits de entrada ---\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    bars1 = ax1.bar(range(3), logits, color=colores, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "    ax1.set_title('Paso 1: Logits (Salida Cruda)', fontsize=12, fontweight='bold', pad=10)\n",
    "    ax1.set_ylabel('Valor', fontsize=10)\n",
    "    ax1.set_xticks(range(3))\n",
    "    ax1.set_xticklabels(clases, fontsize=10)\n",
    "    ax1.axhline(y=0, color='black', linestyle='--', linewidth=0.8, alpha=0.5)\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Ajustar ylim para dar espacio a los textos\n",
    "    y_range = max(logits) - min(logits)\n",
    "    ax1.set_ylim([min(logits) - y_range*0.4, max(logits) + y_range*0.4])\n",
    "    \n",
    "    for i, (bar, val) in enumerate(zip(bars1, logits)):\n",
    "        height = bar.get_height()\n",
    "        offset = y_range*0.08 if height > 0 else -y_range*0.08\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, height + offset, \n",
    "                f'{val:.2f}', ha='center', va='bottom' if height > 0 else 'top',\n",
    "                fontweight='bold', fontsize=10)\n",
    "    \n",
    "    # --- GRAFICA 2: Exponenciales ---\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    bars2 = ax2.bar(range(3), exp_values, color=colores, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "    ax2.set_title('Paso 2: Exponenciales exp(z)', fontsize=12, fontweight='bold', pad=10)\n",
    "    ax2.set_ylabel('Valor', fontsize=10)\n",
    "    ax2.set_xticks(range(3))\n",
    "    ax2.set_xticklabels(clases, fontsize=10)\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Ajustar ylim para dar espacio\n",
    "    ax2.set_ylim([0, max(exp_values) * 1.35])\n",
    "    \n",
    "    for i, (bar, val) in enumerate(zip(bars2, exp_values)):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(exp_values)*0.05, \n",
    "                f'{val:.2f}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    # Mostrar suma (posicionada arriba sin solaparse)\n",
    "    ax2.text(1, max(exp_values)*1.2, f'Suma = {suma_exp:.2f}', \n",
    "            bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7, pad=0.5),\n",
    "            fontsize=10, fontweight='bold', ha='center', va='center')\n",
    "    \n",
    "    # --- GRAFICA 3: Probabilidades finales ---\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    bars3 = ax3.bar(range(3), probabilidades, color=colores, alpha=0.7, \n",
    "                    edgecolor='black', linewidth=2)\n",
    "    ax3.set_title('Paso 3: Probabilidades (Softmax)', fontsize=12, fontweight='bold', pad=10)\n",
    "    ax3.set_ylabel('Probabilidad', fontsize=10)\n",
    "    ax3.set_xticks(range(3))\n",
    "    ax3.set_xticklabels(clases, fontsize=10)\n",
    "    ax3.set_ylim([0, 1.2])\n",
    "    ax3.axhline(y=1.0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for i, (bar, val) in enumerate(zip(bars3, probabilidades)):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.03, \n",
    "                f'{val:.1%}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    # Mostrar suma (posicionada arriba sin solaparse)\n",
    "    suma_prob = np.sum(probabilidades)\n",
    "    ax3.text(1, 1.1, f'Suma = {suma_prob:.4f}', \n",
    "            bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7, pad=0.5),\n",
    "            fontsize=10, fontweight='bold', ha='center', va='center')\n",
    "    \n",
    "    # --- GRAFICA 4: Diagrama de flujo ---\n",
    "    ax4 = fig.add_subplot(gs[1, :])\n",
    "    ax4.set_xlim(0, 10)\n",
    "    ax4.set_ylim(0, 3)\n",
    "    ax4.axis('off')\n",
    "    ax4.set_title('Proceso Completo de Softmax', fontsize=13, fontweight='bold', pad=15)\n",
    "    \n",
    "    # Ecuación paso a paso\n",
    "    y_pos = 1.8\n",
    "    ax4.text(1, y_pos, 'Logits\\n(z)', ha='center', va='center', \n",
    "            fontsize=10, bbox=dict(boxstyle='round', facecolor='#FFB6C1', alpha=0.7))\n",
    "    ax4.annotate('', xy=(1.8, y_pos), xytext=(1.4, y_pos),\n",
    "                arrowprops=dict(arrowstyle='->', lw=2, color='black'))\n",
    "    \n",
    "    ax4.text(2.5, y_pos, 'z - max(z)', ha='center', va='center', \n",
    "            fontsize=9, bbox=dict(boxstyle='round', facecolor='#DDA0DD', alpha=0.7))\n",
    "    ax4.annotate('', xy=(3.5, y_pos), xytext=(3.0, y_pos),\n",
    "                arrowprops=dict(arrowstyle='->', lw=2, color='black'))\n",
    "    \n",
    "    ax4.text(4.5, y_pos, 'exp(z\\')', ha='center', va='center', \n",
    "            fontsize=10, bbox=dict(boxstyle='round', facecolor='#87CEEB', alpha=0.7))\n",
    "    ax4.annotate('', xy=(5.5, y_pos), xytext=(5.1, y_pos),\n",
    "                arrowprops=dict(arrowstyle='->', lw=2, color='black'))\n",
    "    \n",
    "    ax4.text(6.8, y_pos, 'Dividir\\npor Suma', ha='center', va='center', \n",
    "            fontsize=9, bbox=dict(boxstyle='round', facecolor='#FFD700', alpha=0.7))\n",
    "    ax4.annotate('', xy=(8.0, y_pos), xytext=(7.5, y_pos),\n",
    "                arrowprops=dict(arrowstyle='->', lw=2, color='black'))\n",
    "    \n",
    "    ax4.text(9, y_pos, 'Probabilidades\\nSuma = 1', ha='center', va='center', \n",
    "            fontsize=10, bbox=dict(boxstyle='round', facecolor='#90EE90', alpha=0.7))\n",
    "    \n",
    "    # Notas (bien separadas verticalmente)\n",
    "    ax4.text(2.5, 0.7, 'Prevencion de overflow', ha='center', fontsize=9, \n",
    "            style='italic', color='purple')\n",
    "    ax4.text(6.8, 0.7, 'Normalizacion', ha='center', fontsize=9, \n",
    "            style='italic', color='darkblue')\n",
    "    \n",
    "    if temperatura != 1.0:\n",
    "        ax4.text(5, 0.1, f'Temperatura = {temperatura:.2f}', \n",
    "                ha='center', fontsize=9, fontweight='bold', color='darkred',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.6))\n",
    "    \n",
    "    # --- GRAFICA 5: Comparación de temperaturas ---\n",
    "    ax5 = fig.add_subplot(gs[2, :])\n",
    "    temperaturas = [0.5, 1.0, 2.0, 5.0]\n",
    "    x = np.arange(3)\n",
    "    width = 0.18\n",
    "    \n",
    "    for i, T in enumerate(temperaturas):\n",
    "        probs_temp = softmax_con_temp(logits, T)\n",
    "        offset = (i - 1.5) * width\n",
    "        ax5.bar(x + offset, probs_temp, width, label=f'T={T}', alpha=0.7)\n",
    "    \n",
    "    ax5.set_ylabel('Probabilidad', fontsize=10)\n",
    "    ax5.set_title('Efecto de la Temperatura en Softmax', fontsize=12, fontweight='bold', pad=10)\n",
    "    ax5.set_xticks(x)\n",
    "    ax5.set_xticklabels(clases)\n",
    "    ax5.legend(fontsize=9, title='Temperatura', title_fontsize=10, loc='upper left')\n",
    "    ax5.grid(axis='y', alpha=0.3)\n",
    "    ax5.set_ylim([0, 1.05])\n",
    "    ax5.axhline(y=1/3, color='gray', linestyle=':', linewidth=1, alpha=0.5)\n",
    "    \n",
    "    # --- GRAFICA 6: Heatmap de transformación ---\n",
    "    ax6 = fig.add_subplot(gs[3, 0])\n",
    "    data_matrix = np.vstack([logits, exp_values, probabilidades]).T\n",
    "    im = ax6.imshow(data_matrix, cmap='YlOrRd', aspect='auto')\n",
    "    ax6.set_title('Transformacion por Paso', fontsize=11, fontweight='bold', pad=10)\n",
    "    ax6.set_xticks([0, 1, 2])\n",
    "    ax6.set_xticklabels(['Logits', 'Exp', 'Softmax'], fontsize=9)\n",
    "    ax6.set_yticks([0, 1, 2])\n",
    "    ax6.set_yticklabels(clases, fontsize=9)\n",
    "    \n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            color = 'white' if data_matrix[i, j] > np.max(data_matrix)*0.6 else 'black'\n",
    "            ax6.text(j, i, f'{data_matrix[i, j]:.2f}', \n",
    "                    ha='center', va='center', color=color, fontweight='bold', fontsize=9)\n",
    "    \n",
    "    plt.colorbar(im, ax=ax6, fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # --- GRAFICA 7: Fórmula matemática ---\n",
    "    ax7 = fig.add_subplot(gs[3, 1:])\n",
    "    ax7.set_xlim(0, 10)\n",
    "    ax7.set_ylim(0, 4.5)\n",
    "    ax7.axis('off')\n",
    "    \n",
    "    # Título\n",
    "    ax7.text(5, 4.0, 'Formula de Softmax', ha='center', fontsize=13, fontweight='bold')\n",
    "    \n",
    "    # Fórmula principal\n",
    "    formula = r'$\\text{Softmax}(z_i) = \\frac{e^{z_i - \\max(z)}}{\\sum_{j=1}^{n} e^{z_j - \\max(z)}}$'\n",
    "    ax7.text(5, 3.0, formula, ha='center', fontsize=18, \n",
    "            bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "    \n",
    "    # Ejemplo numérico con el primer valor\n",
    "    ax7.text(5, 2.0, 'Ejemplo para Gato:', ha='center', fontsize=11, fontweight='bold')\n",
    "    ejemplo = f'Softmax({logits[0]:.2f}) = {exp_values[0]:.3f} / {suma_exp:.3f} = {probabilidades[0]:.4f}'\n",
    "    ax7.text(5, 1.5, ejemplo, ha='center', fontsize=10, \n",
    "            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.6))\n",
    "    \n",
    "    # Propiedades clave (bien espaciadas)\n",
    "    ax7.text(5, 0.9, 'Salidas entre 0 y 1', ha='center', fontsize=10, \n",
    "            color='darkgreen', fontweight='bold')\n",
    "    ax7.text(5, 0.5, 'La suma de todas las salidas = 1', ha='center', fontsize=10, \n",
    "            color='darkgreen', fontweight='bold')\n",
    "    ax7.text(5, 0.1, 'Numericamente estable (no overflow)', ha='center', fontsize=10, \n",
    "            color='darkgreen', fontweight='bold')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Información numérica\n",
    "    print(\"=\" * 80)\n",
    "    print(\"TRANSFORMACION DE SOFTMAX PASO A PASO\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nPASO 1 - LOGITS (Valores de entrada):\")\n",
    "    for i, (clase, logit) in enumerate(zip(clases, logits)):\n",
    "        print(f\"   {clase:8s}: {logit:8.3f}\")\n",
    "    print(f\"\\n   -> Estos son valores sin restriccion (pueden ser negativos, > 1, etc.)\")\n",
    "    \n",
    "    print(f\"\\nPASO 2 - PREVENCION DE OVERFLOW:\")\n",
    "    print(f\"   Maximo de logits: {np.max(logits):.3f}\")\n",
    "    print(f\"   Logits ajustados (z - max):\")\n",
    "    for i, (clase, val) in enumerate(zip(clases, logits_shifted)):\n",
    "        print(f\"      {clase:8s}: {val:8.3f}\")\n",
    "    print(f\"\\n   -> Restar el maximo no cambia el resultado final, solo evita exp(valores grandes)\")\n",
    "    \n",
    "    print(f\"\\nPASO 3 - EXPONENCIALES:\")\n",
    "    for i, (clase, exp_val) in enumerate(zip(clases, exp_values)):\n",
    "        print(f\"   exp({logits_shifted[i]:6.3f}) = {exp_val:8.3f}  [{clase}]\")\n",
    "    print(f\"\\n   Suma de exponenciales: {suma_exp:.3f}\")\n",
    "    print(f\"   -> Todos los valores ahora son positivos\")\n",
    "    \n",
    "    print(f\"\\nPASO 4 - NORMALIZACION (Dividir por la suma):\")\n",
    "    for clase, exp_val, prob in zip(clases, exp_values, probabilidades):\n",
    "        barra = '█' * int(prob * 50)\n",
    "        print(f\"   {clase:8s}: {exp_val:6.3f} / {suma_exp:.3f} = {prob:.4f} ({prob:6.2%}) {barra}\")\n",
    "    \n",
    "    print(f\"\\n   Suma total: {np.sum(probabilidades):.6f} (aprox. 1.0)\")\n",
    "    print(f\"   -> Ahora los valores estan entre 0 y 1 y suman 1 (son probabilidades)\")\n",
    "    \n",
    "    if temperatura != 1.0:\n",
    "        print(f\"\\nTEMPERATURA: {temperatura}\")\n",
    "        if temperatura < 1:\n",
    "            print(\"   -> T < 1: Las diferencias entre clases se amplifican\")\n",
    "            print(\"   -> La clase ganadora tiene probabilidad mas alta\")\n",
    "        elif temperatura > 1:\n",
    "            print(\"   -> T > 1: Las diferencias se suavizan\")\n",
    "            print(\"   -> Las probabilidades son mas parecidas entre si\")\n",
    "    \n",
    "    print(f\"\\nINTERPRETACION:\")\n",
    "    clase_max = clases[np.argmax(probabilidades)]\n",
    "    prob_max = np.max(probabilidades)\n",
    "    print(f\"   La clase con mayor probabilidad es: {clase_max} con {prob_max:.1%}\")\n",
    "    print(f\"   Esto significa que el modelo 'cree' que la entrada es mas probable que sea {clase_max}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Crear controles\n",
    "slider_temp = widgets.FloatSlider(\n",
    "    value=1.0, min=0.1, max=5.0, step=0.1,\n",
    "    description='Temperatura:',\n",
    "    continuous_update=False,\n",
    "    style={'description_width': '120px'}\n",
    ")\n",
    "\n",
    "slider_magnitud = widgets.FloatSlider(\n",
    "    value=1.0, min=0.5, max=3.0, step=0.1,\n",
    "    description='Magnitud Logits:',\n",
    "    continuous_update=False,\n",
    "    style={'description_width': '120px'}\n",
    ")\n",
    "\n",
    "# Interfaz interactiva\n",
    "print(\"VISUALIZACION INTERACTIVA DE SOFTMAX\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Ajusta los parametros para explorar como Softmax transforma logits en probabilidades:\\n\")\n",
    "print(\"* Temperatura: Controla que tan 'suave' o 'picuda' es la distribucion\")\n",
    "print(\"* Magnitud Logits: Cambia la escala de los valores de entrada\\n\")\n",
    "\n",
    "widgets.interact(visualizar_softmax,\n",
    "                temperatura=slider_temp,\n",
    "                magnitud_logits=slider_magnitud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec64678",
   "metadata": {},
   "source": [
    "## Calculo de perdidas\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d305bb9-18eb-4334-a159-f8fbc717d775",
   "metadata": {},
   "source": [
    "## Función de Error (Loss) y Entropía Cruzada Categórica\n",
    "\n",
    "Hasta ahora, nuestra red neuronal ya puede **calcular salidas y probabilidades** gracias a Softmax.  \n",
    "\n",
    "Pero... ¿cómo sabemos si esas salidas están \"bien\" o \"mal\"?  \n",
    "\n",
    "Ahí entra la **función de error (o pérdida)**.\n",
    "\n",
    "\n",
    "### ¿Por qué medimos el error y no la precisión?\n",
    "\n",
    "Podríamos intentar medir la **precisión** (porcentaje de aciertos), pero esa medida es **demasiado gruesa**:  \n",
    "solo nos dice si una predicción es correcta o no, sin dar información sobre *cuánto* se equivocó la red.\n",
    "\n",
    "Por ejemplo:  \n",
    "- Si la red predice `[0.6, 0.4]` para una clase real `[1, 0]`, no está del todo mal.  \n",
    "- Pero si predice `[0.51, 0.49]`, está muy insegura, aunque la precisión sea igual (acertó).  \n",
    "\n",
    "Para **aprender correctamente**, necesitamos una medida continua que capture esa diferencia.  \n",
    "Esa medida es la **función de pérdida**, que le da al modelo una forma cuantitativa de saber *cuánto se está equivocando*.\n",
    "\n",
    "\n",
    "### ¿Qué es la Entropía Cruzada?\n",
    "\n",
    "La **entropía cruzada categórica (Categorical Cross-Entropy)** mide la diferencia entre la **distribución real** de las clases y la **distribución predicha** por el modelo.\n",
    "\n",
    "Matemáticamente:\n",
    "\n",
    "$$\n",
    "L = -\\sum_i y_i \\cdot \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $y_i$ es la etiqueta real (1 para la clase correcta, 0 para las demás).\n",
    "- $\\hat{y}_i$ es la probabilidad predicha por la red para esa clase.\n",
    "\n",
    "Esta función penaliza más cuando el modelo da una **baja probabilidad a la clase correcta**, y menos cuando se acerca a 1.\n",
    "\n",
    "\n",
    "### Intuición\n",
    "\n",
    "Imagina que tu red tiene que decidir entre tres clases, y la respuesta correcta es la clase 1.  \n",
    "\n",
    "| Predicción | Interpretación | Pérdida |\n",
    "|-------------|----------------|---------|\n",
    "| [0.9, 0.05, 0.05] | Confía en la clase correcta | Baja |\n",
    "| [0.33, 0.33, 0.34] | Duda mucho | Media |\n",
    "| [0.05, 0.9, 0.05] | Segura pero incorrecta | Alta |\n",
    "\n",
    "Así, la red aprende a **aumentar la probabilidad de la clase correcta** y disminuir las otras.\n",
    "\n",
    "\n",
    "\n",
    "### En resumen\n",
    "\n",
    "- El **error (loss)** guía el aprendizaje de la red.  \n",
    "- La **precisión** sirve para evaluar resultados, pero no para entrenar.  \n",
    "- La **entropía cruzada categórica** es la más usada cuando hay múltiples clases de salida.  \n",
    "- Se combina naturalmente con **Softmax**, porque Softmax produce una distribución de probabilidades.\n",
    "\n",
    "\n",
    "\n",
    "## Ejemplo en Python\n",
    "\n",
    "A continuación implementamos la **entropía cruzada categórica** y la usamos para calcular el error entre las salidas de Softmax y las clases reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd4f2a69-7f38-45c3-ba78-55d794e08be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones (probabilidades):\n",
      "[[0.30326886 0.40108669 0.29564444]\n",
      " [0.32834771 0.36829707 0.30335522]\n",
      " [0.31750561 0.37292873 0.30956567]]\n",
      "\n",
      "Pérdidas individuales:\n",
      "[1.19313553 0.99886542 0.98636796]\n",
      "\n",
      "Pérdida promedio total: 1.0595\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ======== Clases previas ========\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Pesos: matriz (n_inputs × n_neurons)\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "        # Biases: vector fila (1 × n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):\n",
    "        # Producto punto + bias\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        # ReLU: reemplaza valores negativos por 0\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        # Estabilización numérica restando el máximo\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # Normalización por el total de exponentes\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "# ======== Clase de función de pérdida ========\n",
    "\n",
    "class Loss_CategoricalCrossentropy:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Evitar log(0) o valores extremos\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Si las etiquetas son one-hot (matriz)\n",
    "        if len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "        # Si las etiquetas son índices (vector)\n",
    "        elif len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(len(y_pred)), y_true]\n",
    "\n",
    "        # Pérdida individual por muestra\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "# ======== Ejemplo práctico ========\n",
    "\n",
    "# Entradas: 3 muestras con 4 características\n",
    "X = np.array([\n",
    "    [1.0, 2.0, 3.0, 2.5],\n",
    "    [2.0, 5.0, -1.0, 2.0],\n",
    "    [-1.5, 2.7, 3.3, -0.8]\n",
    "])\n",
    "\n",
    "# Etiquetas reales: clases correctas (0, 1, 1)\n",
    "y_true = np.array([0, 1, 1])   # <--- convertimos a array\n",
    "\n",
    "# Definición de la red\n",
    "np.random.seed(0)\n",
    "layer1 = Layer_Dense(4, 5)\n",
    "activation1 = Activation_ReLU()\n",
    "layer2 = Layer_Dense(5, 3)\n",
    "activation2 = Activation_Softmax()\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Forward pass\n",
    "layer1.forward(X)\n",
    "activation1.forward(layer1.output)\n",
    "layer2.forward(activation1.output)\n",
    "activation2.forward(layer2.output)\n",
    "\n",
    "# Cálculo de la pérdida\n",
    "loss = loss_function.forward(activation2.output, y_true)\n",
    "average_loss = np.mean(loss)\n",
    "\n",
    "print(\"Predicciones (probabilidades):\")\n",
    "print(activation2.output)\n",
    "print(\"\\nPérdidas individuales:\")\n",
    "print(loss)\n",
    "print(f\"\\nPérdida promedio total: {average_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a357247d-7301-4445-91d6-fd45a8e778ac",
   "metadata": {},
   "source": [
    "Ahora un ejemplo interactivo de la cross-entropía:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "777e1be1-8c61-4792-b8a9-3162eef673cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ajusta los parametros para ver como cambia el loss segun las predicciones\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6beb4301d94a6c842f4b13f66d0e1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, continuous_update=False, description='Mult. Pesos L1:', max=3.0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.visualizar_loss(mult_pesos1, mult_pesos2, seed)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ======== Clases de la red ========\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "class Loss_CategoricalCrossentropy:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        if len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "        elif len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(len(y_pred)), y_true]\n",
    "        \n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "def visualizar_loss(mult_pesos1, mult_pesos2, seed):\n",
    "    \"\"\"Visualiza el cálculo de loss y cross-entropy\"\"\"\n",
    "    \n",
    "    # Datos de entrada\n",
    "    X = np.array([\n",
    "        [1.0, 2.0, 3.0, 2.5],\n",
    "        [2.0, 5.0, -1.0, 2.0],\n",
    "        [-1.5, 2.7, 3.3, -0.8]\n",
    "    ])\n",
    "    \n",
    "    # Etiquetas verdaderas\n",
    "    y_true = np.array([0, 1, 1])\n",
    "    nombres_clases = ['Clase 0', 'Clase 1', 'Clase 2']\n",
    "    n_muestras = len(X)\n",
    "    n_clases = 3\n",
    "    \n",
    "    # Configurar red\n",
    "    np.random.seed(seed)\n",
    "    layer1 = Layer_Dense(4, 5)\n",
    "    layer1.weights *= mult_pesos1\n",
    "    activation1 = Activation_ReLU()\n",
    "    layer2 = Layer_Dense(5, 3)\n",
    "    layer2.weights *= mult_pesos2\n",
    "    activation2 = Activation_Softmax()\n",
    "    loss_function = Loss_CategoricalCrossentropy()\n",
    "    \n",
    "    # Forward pass\n",
    "    layer1.forward(X)\n",
    "    activation1.forward(layer1.output)\n",
    "    layer2.forward(activation1.output)\n",
    "    activation2.forward(layer2.output)\n",
    "    \n",
    "    # Calcular pérdida\n",
    "    loss = loss_function.forward(activation2.output, y_true)\n",
    "    average_loss = np.mean(loss)\n",
    "    \n",
    "    # Calcular precisión\n",
    "    predictions = np.argmax(activation2.output, axis=1)\n",
    "    accuracy = np.mean(predictions == y_true)\n",
    "    \n",
    "    # Cerrar figuras previas\n",
    "    plt.close('all')\n",
    "    \n",
    "    # Crear visualización\n",
    "    fig = plt.figure(figsize=(16, 13))\n",
    "    gs = fig.add_gridspec(4, 3, hspace=0.5, wspace=0.35, top=0.96, bottom=0.04)\n",
    "    \n",
    "    # --- GRÁFICA 1: Heatmap de probabilidades predichas ---\n",
    "    ax1 = fig.add_subplot(gs[0, :2])\n",
    "    im1 = ax1.imshow(activation2.output, cmap='YlGnBu', aspect='auto', vmin=0, vmax=1)\n",
    "    ax1.set_title('Probabilidades Predichas por la Red (Salida de Softmax)', \n",
    "                  fontsize=12, fontweight='bold', pad=20)\n",
    "    ax1.set_xlabel('Clase', fontsize=10)\n",
    "    ax1.set_ylabel('Muestra', fontsize=10)\n",
    "    ax1.set_xticks(range(n_clases))\n",
    "    ax1.set_yticks(range(n_muestras))\n",
    "    ax1.set_xticklabels(nombres_clases, fontsize=9)\n",
    "    ax1.set_yticklabels([f'Muestra {i}' for i in range(n_muestras)], fontsize=9)\n",
    "    \n",
    "    # Añadir valores y marcar clases correctas\n",
    "    for i in range(n_muestras):\n",
    "        for j in range(n_clases):\n",
    "            value = activation2.output[i, j]\n",
    "            color = 'white' if value > 0.5 else 'black'\n",
    "            \n",
    "            # Marcar la clase correcta con un borde\n",
    "            if j == y_true[i]:\n",
    "                rect = plt.Rectangle((j-0.45, i-0.45), 0.9, 0.9, \n",
    "                                    fill=False, edgecolor='red', linewidth=3)\n",
    "                ax1.add_patch(rect)\n",
    "                ax1.text(j, i, f'{value:.3f}\\n(REAL)', ha='center', va='center', \n",
    "                        color=color, fontsize=8, fontweight='bold')\n",
    "            else:\n",
    "                ax1.text(j, i, f'{value:.3f}', ha='center', va='center', \n",
    "                        color=color, fontsize=8)\n",
    "    \n",
    "    plt.colorbar(im1, ax=ax1, label='Probabilidad', fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # --- GRÁFICA 2: Curva teórica de Cross-Entropy ---\n",
    "    ax2 = fig.add_subplot(gs[0, 2])\n",
    "    probs = np.linspace(0.01, 1, 1000)\n",
    "    ce_values = -np.log(probs)\n",
    "    \n",
    "    ax2.plot(probs, ce_values, linewidth=2.5, color='#E74C3C')\n",
    "    ax2.fill_between(probs, ce_values, alpha=0.3, color='#E74C3C')\n",
    "    ax2.set_xlabel('Probabilidad de la clase correcta', fontsize=9)\n",
    "    ax2.set_ylabel('Loss (Cross-Entropy)', fontsize=9)\n",
    "    ax2.set_title('Curva de Penalizacion\\n-log(p)', fontsize=11, fontweight='bold', pad=15)\n",
    "    ax2.grid(alpha=0.3)\n",
    "    ax2.set_ylim([0, 5])\n",
    "    \n",
    "    # Marcar puntos de referencia\n",
    "    ref_probs = [0.1, 0.5, 0.9]\n",
    "    for p in ref_probs:\n",
    "        loss_val = -np.log(p)\n",
    "        ax2.plot(p, loss_val, 'o', markersize=8, color='darkred')\n",
    "        ax2.text(p, loss_val + 0.3, f'p={p}\\nL={loss_val:.2f}', \n",
    "                ha='center', fontsize=7, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\n",
    "    \n",
    "    # --- GRÁFICA 3: Comparación por muestra ---\n",
    "    ax3 = fig.add_subplot(gs[1, :])\n",
    "    x = np.arange(n_muestras)\n",
    "    width = 0.25\n",
    "    \n",
    "    # Barras para cada clase\n",
    "    colores_clases = ['#3498DB', '#2ECC71', '#F39C12']\n",
    "    for j in range(n_clases):\n",
    "        offset = (j - 1) * width\n",
    "        bars = ax3.bar(x + offset, activation2.output[:, j], width, \n",
    "                      label=nombres_clases[j], color=colores_clases[j], alpha=0.8)\n",
    "        \n",
    "        # Resaltar la clase correcta para cada muestra\n",
    "        for i in range(n_muestras):\n",
    "            if j == y_true[i]:\n",
    "                bars[i].set_edgecolor('red')\n",
    "                bars[i].set_linewidth(3)\n",
    "    \n",
    "    ax3.set_xlabel('Muestra', fontsize=10)\n",
    "    ax3.set_ylabel('Probabilidad', fontsize=10)\n",
    "    ax3.set_title('Distribucion de Probabilidades por Muestra (Borde rojo = Clase correcta)', \n",
    "                  fontsize=12, fontweight='bold', pad=20)\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels([f'M{i}\\n(Real: {y_true[i]})' for i in range(n_muestras)], fontsize=9)\n",
    "    ax3.legend(loc='upper right', fontsize=9)\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    ax3.set_ylim([0, 1.15])\n",
    "    \n",
    "    # Línea en 1.0 para mostrar que suman 1\n",
    "    ax3.axhline(y=1.0, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    \n",
    "    # --- GRÁFICA 4: Pérdida individual por muestra ---\n",
    "    ax4 = fig.add_subplot(gs[2, 0])\n",
    "    colores_loss = ['#27AE60' if l < 0.5 else '#E67E22' if l < 1.5 else '#E74C3C' for l in loss]\n",
    "    bars4 = ax4.bar(range(n_muestras), loss, color=colores_loss, alpha=0.8, edgecolor='black')\n",
    "    ax4.set_xlabel('Muestra', fontsize=10)\n",
    "    ax4.set_ylabel('Loss Individual', fontsize=10)\n",
    "    ax4.set_title('Perdida por Muestra', fontsize=11, fontweight='bold', pad=15)\n",
    "    ax4.set_xticks(range(n_muestras))\n",
    "    ax4.set_xticklabels([f'M{i}' for i in range(n_muestras)], fontsize=9)\n",
    "    ax4.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Calcular límites con margen\n",
    "    max_loss = np.max(loss)\n",
    "    ax4.set_ylim([0, max_loss * 1.25])\n",
    "    \n",
    "    # Añadir valores sobre las barras\n",
    "    for i, (bar, l) in enumerate(zip(bars4, loss)):\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height + max_loss * 0.03,\n",
    "                f'{l:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    # Línea del promedio\n",
    "    ax4.axhline(y=average_loss, color='red', linestyle='--', linewidth=2, \n",
    "               label=f'Promedio: {average_loss:.3f}')\n",
    "    ax4.legend(fontsize=8)\n",
    "    \n",
    "    # --- GRÁFICA 5: Confianza en la clase correcta ---\n",
    "    ax5 = fig.add_subplot(gs[2, 1])\n",
    "    confidencias = activation2.output[range(n_muestras), y_true]\n",
    "    colores_conf = ['#27AE60' if c > 0.7 else '#E67E22' if c > 0.4 else '#E74C3C' for c in confidencias]\n",
    "    bars5 = ax5.bar(range(n_muestras), confidencias, color=colores_conf, alpha=0.8, edgecolor='black')\n",
    "    ax5.set_xlabel('Muestra', fontsize=10)\n",
    "    ax5.set_ylabel('Probabilidad', fontsize=10)\n",
    "    ax5.set_title('Confianza en la Clase Correcta', fontsize=11, fontweight='bold', pad=15)\n",
    "    ax5.set_xticks(range(n_muestras))\n",
    "    ax5.set_xticklabels([f'M{i}' for i in range(n_muestras)], fontsize=9)\n",
    "    ax5.set_ylim([0, 1.15])\n",
    "    ax5.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Añadir valores\n",
    "    for i, (bar, c) in enumerate(zip(bars5, confidencias)):\n",
    "        height = bar.get_height()\n",
    "        ax5.text(bar.get_x() + bar.get_width()/2., height + 0.03,\n",
    "                f'{c:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    # Zonas de referencia\n",
    "    ax5.axhspan(0.7, 1.0, alpha=0.1, color='green', label='Confiado')\n",
    "    ax5.axhspan(0.4, 0.7, alpha=0.1, color='orange', label='Dudoso')\n",
    "    ax5.axhspan(0, 0.4, alpha=0.1, color='red', label='Inseguro')\n",
    "    ax5.legend(fontsize=7, loc='lower right')\n",
    "    \n",
    "    # --- GRÁFICA 6: Predicciones vs Verdad ---\n",
    "    ax6 = fig.add_subplot(gs[2, 2])\n",
    "    ax6.axis('off')\n",
    "    ax6.set_title('Predicciones vs Realidad', fontsize=11, fontweight='bold', pad=15)\n",
    "    ax6.set_xlim([0, 1])\n",
    "    ax6.set_ylim([0, 1])\n",
    "    \n",
    "    y_text = 0.95\n",
    "    for i in range(n_muestras):\n",
    "        pred_class = predictions[i]\n",
    "        true_class = y_true[i]\n",
    "        pred_prob = activation2.output[i, pred_class]\n",
    "        true_prob = activation2.output[i, true_class]\n",
    "        \n",
    "        is_correct = pred_class == true_class\n",
    "        color = 'green' if is_correct else 'red'\n",
    "        symbol = 'OK' if is_correct else 'ERROR'\n",
    "        \n",
    "        text = f'Muestra {i}:\\n'\n",
    "        text += f'  Pred: Clase {pred_class} (p={pred_prob:.3f})\\n'\n",
    "        text += f'  Real: Clase {true_class} (p={true_prob:.3f})\\n'\n",
    "        text += f'  [{symbol}] Loss: {loss[i]:.3f}'\n",
    "        \n",
    "        ax6.text(0.05, y_text, text, fontsize=8, family='monospace',\n",
    "                bbox=dict(boxstyle='round', facecolor=color, alpha=0.2),\n",
    "                verticalalignment='top', transform=ax6.transAxes)\n",
    "        y_text -= 0.30\n",
    "    \n",
    "    # --- GRÁFICA 7: Métricas generales ---\n",
    "    ax7 = fig.add_subplot(gs[3, :])\n",
    "    ax7.axis('off')\n",
    "    ax7.set_xlim([0, 1])\n",
    "    ax7.set_ylim([0, 1])\n",
    "    \n",
    "    # Cuadro de métricas\n",
    "    metricas_text = f'''METRICAS DE LA RED\n",
    "══════════════════════════════════════════════════════════════\n",
    "LOSS (Perdida):\n",
    "  - Promedio:         {average_loss:.4f}\n",
    "  - Minimo (mejor):   {np.min(loss):.4f}  (Muestra {np.argmin(loss)})\n",
    "  - Maximo (peor):    {np.max(loss):.4f}  (Muestra {np.argmax(loss)})\n",
    "\n",
    "PRECISION (Accuracy):\n",
    "  - Aciertos:         {int(accuracy * n_muestras)}/{n_muestras}\n",
    "  - Porcentaje:       {accuracy * 100:.1f}%\n",
    "\n",
    "INTERPRETACION DEL LOSS:\n",
    "  - BAJO  (<0.5):  Red muy confiada en la clase correcta\n",
    "  - MEDIO (0.5-1.5): Red con dudas, pero razonable\n",
    "  - ALTO  (>1.5):  Red equivocada o muy insegura\n",
    "\n",
    "DIFERENCIA CLAVE:\n",
    "  - PRECISION: Solo cuenta si acierta o no (binario: 0 o 1)\n",
    "  - LOSS: Mide CUAN segura esta (continuo, diferenciable)\n",
    "  \n",
    "  Usamos LOSS para entrenar (permite gradientes)\n",
    "  y PRECISION para evaluar resultados finales.'''\n",
    "    \n",
    "    ax7.text(0.5, 0.5, metricas_text, fontsize=9, family='monospace',\n",
    "            ha='center', va='center', transform=ax7.transAxes,\n",
    "            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Información en consola\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ANALISIS DETALLADO DE LOSS Y CROSS-ENTROPY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nPROBABILIDADES PREDICHAS:\")\n",
    "    print(activation2.output)\n",
    "    print(f\"\\nCLASES VERDADERAS: {y_true}\")\n",
    "    print(f\"CLASES PREDICHAS:  {predictions}\")\n",
    "    print(f\"\\nCONFIANZA EN CLASE CORRECTA:\")\n",
    "    for i in range(n_muestras):\n",
    "        print(f\"  Muestra {i}: {confidencias[i]:.4f} -> Loss: {loss[i]:.4f}\")\n",
    "    print(f\"\\nLOSS PROMEDIO:     {average_loss:.4f}\")\n",
    "    print(f\"PRECISION:         {accuracy*100:.1f}%\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Crear sliders\n",
    "slider_pesos1 = widgets.FloatSlider(\n",
    "    value=1.0, min=0.1, max=3.0, step=0.1,\n",
    "    description='Mult. Pesos L1:',\n",
    "    continuous_update=False,\n",
    "    style={'description_width': '120px'}\n",
    ")\n",
    "\n",
    "slider_pesos2 = widgets.FloatSlider(\n",
    "    value=1.0, min=0.1, max=3.0, step=0.1,\n",
    "    description='Mult. Pesos L2:',\n",
    "    continuous_update=False,\n",
    "    style={'description_width': '120px'}\n",
    ")\n",
    "\n",
    "slider_seed = widgets.IntSlider(\n",
    "    value=0, min=0, max=10, step=1,\n",
    "    description='Semilla:',\n",
    "    continuous_update=False,\n",
    "    style={'description_width': '120px'}\n",
    ")\n",
    "\n",
    "# Crear interfaz interactiva\n",
    "print(\"Ajusta los parametros para ver como cambia el loss segun las predicciones\\n\")\n",
    "widgets.interact(visualizar_loss,\n",
    "                mult_pesos1=slider_pesos1,\n",
    "                mult_pesos2=slider_pesos2,\n",
    "                seed=slider_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d763707",
   "metadata": {},
   "source": [
    "## Optimizacion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35af1ea3-9384-42e3-a4c3-d62a67c90bd1",
   "metadata": {},
   "source": [
    "## Optimización y ajuste de parámetros\n",
    "\n",
    "Hasta este punto, hemos visto cómo una red neuronal calcula sus salidas a partir de las entradas, los **pesos** y los **sesgos**.\n",
    "\n",
    "Sin embargo, el verdadero poder de las redes neuronales radica en su capacidad de **aprender automáticamente** los valores de esos parámetros a través de un proceso conocido como **optimización**.\n",
    "\n",
    "\n",
    "### ¿Por qué necesitamos optimizadores?\n",
    "\n",
    "En cada iteración del entrenamiento, la red produce una predicción y la compara con el valor real (la etiqueta verdadera).\n",
    "\n",
    "A partir de esta comparación se calcula una **función de pérdida**, que mide qué tan lejos está la predicción de la verdad.\n",
    "\n",
    "El objetivo del optimizador es **ajustar los pesos y sesgos de manera que la pérdida disminuya**.\n",
    "\n",
    "Dicho de forma intuitiva, el optimizador es el **mecanismo que entrena el modelo**: decide, con base en el error actual, **cómo y en qué dirección** deben modificarse los pesos para mejorar la precisión de la red.\n",
    "\n",
    "\n",
    "### ¿Por qué es un concepto complicado?\n",
    "\n",
    "El problema de optimización de una red neuronal no es trivial:\n",
    "\n",
    "- El espacio de parámetros (todos los pesos y sesgos) puede ser **enorme**.\n",
    "- La superficie de error puede ser **altamente no lineal**, con muchos mínimos locales.\n",
    "- Ajustar un parámetro puede afectar indirectamente a muchos otros.\n",
    "\n",
    "Por eso, los optimizadores modernos (como *SGD*, *Momentum*, *RMSProp* o *Adam*) implementan estrategias matemáticas sofisticadas para **ajustar cada peso de manera inteligente**, sin que nosotros tengamos que resolver manualmente el problema.\n",
    "\n",
    "\n",
    "### Intuición del proceso de ajuste\n",
    "\n",
    "Podemos imaginar el proceso de **optimización** como caminar por una superficie montañosa tratando de alcanzar el punto más bajo, que representa la menor pérdida posible. En cada paso de este recorrido, el modelo analiza la forma de esa superficie y decide cómo ajustar sus parámetros para acercarse al mínimo.\n",
    "\n",
    "En términos generales, el proceso sigue tres pasos:\n",
    "\n",
    "1. Se calcula la **pendiente** de la superficie, es decir, el **gradiente** de la función de pérdida respecto a cada peso.  \n",
    "2. Se actualizan los pesos moviéndose en la dirección contraria a esa pendiente, ya que es la que reduce la pérdida.  \n",
    "3. Se repite el procedimiento hasta que los cambios sean mínimos y la pérdida se estabilice en un valor suficientemente bajo.\n",
    "\n",
    "Matemáticamente, la actualización de un peso $w$ se expresa como:\n",
    "\n",
    "$$\n",
    "w_{\\text{nuevo}} = w_{\\text{viejo}} - \\eta \\frac{\\partial L}{\\partial w}\n",
    "$$\n",
    "\n",
    "donde:  \n",
    "- $\\eta$ es la **tasa de aprendizaje** (*learning rate*), que controla el tamaño de cada paso.  \n",
    "- $\\frac{\\partial L}{\\partial w}$ representa el **gradiente** de la pérdida con respecto al peso.\n",
    "\n",
    "Durante este proceso —también conocido como **entrenamiento de la red neuronal**— se distinguen dos fases principales: el **forward pass** y el **backward pass**. En el *forward pass*, los datos de entrada atraviesan la red capa por capa, aplicando pesos, sesgos y funciones de activación hasta producir una salida o predicción. Luego, en el *backward pass*, se calcula el error entre la predicción y el valor real, y se utiliza el algoritmo de **backpropagation** para propagar ese error hacia atrás por la red. Gracias a este mecanismo, los gradientes obtenidos permiten actualizar los pesos de forma eficiente, haciendo que el modelo aprenda patrones cada vez más precisos y cierre así el ciclo completo de aprendizaje supervisado.\n",
    "\n",
    "### Cómo simplificamos esto en la práctica\n",
    "\n",
    "Implementar estos cálculos desde cero puede ser tedioso y propenso a errores.\n",
    "\n",
    "Por eso, librerías como **NumPy**, **TensorFlow** o **PyTorch** nos permiten centrarnos en la arquitectura y la lógica, mientras ellas se encargan de calcular los gradientes y actualizar los parámetros automáticamente mediante los optimizadores.\n",
    "\n",
    "De hecho, en frameworks modernos basta con definir el optimizador y la función de pérdida; el resto (diferenciación automática, propagación del error y actualización de parámetros) se maneja internamente.\n",
    "\n",
    "### En resumen\n",
    "\n",
    "- Los **optimizadores** son el corazón del proceso de aprendizaje.\n",
    "- Permiten ajustar los pesos y sesgos reduciendo progresivamente la pérdida.\n",
    "- Su papel es fundamental: **sin un optimizador, el modelo nunca aprendería**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37f3d94a-b51e-4e68-99b2-ce4ee2ec1158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENTRENAMIENTO CON OPTIMIZADOR SGD\n",
      "============================================================\n",
      "\n",
      "Pesos iniciales:\n",
      "[[0.17640523]\n",
      " [0.04001572]\n",
      " [0.0978738 ]]\n",
      "\n",
      "Biases iniciales:\n",
      "[[0.]]\n",
      "\n",
      "Epoca 1:\n",
      "  Perdida: 0.6731\n",
      "  Predicciones: [0.63414906 0.70356928 0.76470455 0.81651949]\n",
      "\n",
      "Epoca 2:\n",
      "  Perdida: 1.0311\n",
      "  Predicciones: [0.28193946 0.23312932 0.1905272  0.15414535]\n",
      "\n",
      "Epoca 3:\n",
      "  Perdida: 2.5950\n",
      "  Predicciones: [0.98441377 0.99800727 0.99974826 0.99996825]\n",
      "\n",
      "Epoca 4:\n",
      "  Perdida: 0.5950\n",
      "  Predicciones: [0.42094248 0.47279223 0.52523559 0.57712813]\n",
      "\n",
      "Epoca 5:\n",
      "  Perdida: 0.8526\n",
      "  Predicciones: [0.7305518  0.86585825 0.93890156 0.97339328]\n",
      "\n",
      "============================================================\n",
      "RESULTADOS FINALES\n",
      "============================================================\n",
      "\n",
      "Pesos finales:\n",
      "[[ 0.3088204 ]\n",
      " [-0.23577219]\n",
      " [-0.58611718]]\n",
      "\n",
      "Biases finales:\n",
      "[[-0.40820307]]\n",
      "\n",
      "Predicciones finales:\n",
      "  Muestra 0: pred=0.0887 (Clase 0) | real=Clase 0 | ✓\n",
      "  Muestra 1: pred=0.0551 (Clase 0) | real=Clase 0 | ✓\n",
      "  Muestra 2: pred=0.0337 (Clase 0) | real=Clase 1 | ✗\n",
      "  Muestra 3: pred=0.0205 (Clase 0) | real=Clase 1 | ✗\n",
      "\n",
      "Perdida final: 1.8571\n",
      "\n",
      "Nota: Los pesos se han ajustado automaticamente para reducir la perdida\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Datos de entrada (4 muestras, 3 características cada una)\n",
    "X = np.array([[1.0, 2.0, 3.0],\n",
    "              [2.0, 3.0, 4.0],\n",
    "              [3.0, 4.0, 5.0],\n",
    "              [4.0, 5.0, 6.0]])\n",
    "\n",
    "# Etiquetas verdaderas (clasificación binaria: 0 o 1)\n",
    "y_true = np.array([[0],\n",
    "                   [0],\n",
    "                   [1],\n",
    "                   [1]])\n",
    "\n",
    "# Fijar semilla para reproducibilidad\n",
    "np.random.seed(0)\n",
    "\n",
    "# Capa densa\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Pesos aleatorios pequeños\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "        # Biases inicializados en cero\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Guardar las entradas para usar en backprop\n",
    "        self.inputs = inputs\n",
    "        # Producto punto entre entradas y pesos + biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# Función de activación Sigmoid (para clasificación binaria)\n",
    "class Activation_Sigmoid:\n",
    "    def forward(self, inputs):\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "\n",
    "# Función de pérdida: Binary Cross-Entropy\n",
    "class Loss_BinaryCrossEntropy:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Añadir pequeño epsilon para evitar log(0)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        # Calcular pérdida\n",
    "        loss = -np.mean(y_true * np.log(y_pred_clipped) + \n",
    "                        (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        return loss\n",
    "\n",
    "# Optimizador: Stochastic Gradient Descent (SGD)\n",
    "class Optimizer_SGD:\n",
    "    def __init__(self, learning_rate=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def update_params(self, layer, dweights, dbiases):\n",
    "        # Actualizar pesos: w_nuevo = w_viejo - learning_rate * gradiente\n",
    "        layer.weights -= self.learning_rate * dweights\n",
    "        layer.biases -= self.learning_rate * dbiases\n",
    "\n",
    "# Crear red neuronal simple\n",
    "# 3 entradas -> 1 neurona de salida (clasificación binaria)\n",
    "layer1 = Layer_Dense(3, 1)\n",
    "activation1 = Activation_Sigmoid()\n",
    "loss_function = Loss_BinaryCrossEntropy()\n",
    "optimizer = Optimizer_SGD(learning_rate=0.5)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ENTRENAMIENTO CON OPTIMIZADOR SGD\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nPesos iniciales:\\n{layer1.weights}\")\n",
    "print(f\"\\nBiases iniciales:\\n{layer1.biases}\")\n",
    "\n",
    "# Entrenar por varias épocas\n",
    "n_epochs = 5\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward pass\n",
    "    layer1.forward(X)\n",
    "    activation1.forward(layer1.output)\n",
    "    y_pred = activation1.output\n",
    "    \n",
    "    # Calcular pérdida\n",
    "    loss = loss_function.forward(y_pred, y_true)\n",
    "    \n",
    "    # Calcular gradientes (simplificado para este ejemplo)\n",
    "    # En la práctica, esto se hace con backpropagation\n",
    "    # Gradiente de la pérdida respecto a las predicciones\n",
    "    dloss = y_pred - y_true\n",
    "    \n",
    "    # Gradientes respecto a pesos y biases\n",
    "    dweights = np.dot(layer1.inputs.T, dloss) / len(X)\n",
    "    dbiases = np.mean(dloss, axis=0, keepdims=True)\n",
    "    \n",
    "    # Actualizar parámetros usando el optimizador\n",
    "    optimizer.update_params(layer1, dweights, dbiases)\n",
    "    \n",
    "    # Mostrar progreso\n",
    "    print(f\"\\nEpoca {epoch + 1}:\")\n",
    "    print(f\"  Perdida: {loss:.4f}\")\n",
    "    print(f\"  Predicciones: {y_pred.T.flatten()}\")\n",
    "\n",
    "# Resultados finales\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESULTADOS FINALES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nPesos finales:\\n{layer1.weights}\")\n",
    "print(f\"\\nBiases finales:\\n{layer1.biases}\")\n",
    "\n",
    "# Forward pass final para ver predicciones\n",
    "layer1.forward(X)\n",
    "activation1.forward(layer1.output)\n",
    "y_pred_final = activation1.output\n",
    "loss_final = loss_function.forward(y_pred_final, y_true)\n",
    "\n",
    "print(f\"\\nPredicciones finales:\")\n",
    "for i, (pred, true) in enumerate(zip(y_pred_final, y_true)):\n",
    "    clase_pred = \"Clase 1\" if pred[0] > 0.5 else \"Clase 0\"\n",
    "    clase_true = \"Clase 1\" if true[0] == 1 else \"Clase 0\"\n",
    "    print(f\"  Muestra {i}: pred={pred[0]:.4f} ({clase_pred}) | \"\n",
    "          f\"real={clase_true} | {'✓' if (pred[0] > 0.5) == true[0] else '✗'}\")\n",
    "\n",
    "print(f\"\\nPerdida final: {loss_final:.4f}\")\n",
    "print(\"\\nNota: Los pesos se han ajustado automaticamente para reducir la perdida\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f96f5e",
   "metadata": {},
   "source": [
    "## Implementacion en Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dd63a7-ed42-4f93-a482-3c368ea87340",
   "metadata": {},
   "source": [
    "A lo largo de los capítulos anteriores, se han construido **desde cero** los componentes fundamentales de una red neuronal:\n",
    "\n",
    "- Capas densas con pesos y sesgos\n",
    "- Funciones de activación (ReLU, Softmax)\n",
    "- Procesamiento por batches\n",
    "- Funciones de pérdida\n",
    "- Optimizadores\n",
    "\n",
    "Implementar estos elementos manualmente ha permitido entender **qué está pasando realmente** bajo el capó de una red neuronal.\n",
    "\n",
    "Sin embargo, en la práctica, construir redes desde cero para cada proyecto sería **ineficiente y propenso a errores**.\n",
    "\n",
    "\n",
    "### El rol de las librerías modernas\n",
    "\n",
    "Aquí es donde entran librerías como **TensorFlow/Keras**, **PyTorch** y otras:\n",
    "\n",
    "- **Simplifican el código**: Lo que nos tomó decenas de líneas ahora son unas pocas.\n",
    "- **Optimizan el rendimiento**: Usan implementaciones altamente eficientes en C/C++ y aprovechan GPUs.\n",
    "- **Manejan la complejidad**: Calculan gradientes automáticamente (diferenciación automática).\n",
    "- **Reducen errores**: Código probado y mantenido por miles de desarrolladores.\n",
    "\n",
    "### Comparación: Implementación manual vs TensorFlow\n",
    "\n",
    "Veamos cómo todo lo que se ha construido se traduce a TensorFlow/Keras de forma extremadamente concisa.\n",
    "\n",
    "## Ejemplo en Python\n",
    "\n",
    "A continuación, se implementa **exactamente la misma arquitectura** que se ha venido trabajando, pero usando TensorFlow/Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e634dd84-58ce-4282-a4a3-77dc71bb5750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RED NEURONAL CON TENSORFLOW/KERAS\n",
      "======================================================================\n",
      "\n",
      "Datos de entrada:\n",
      "X shape: (3, 4) (3 muestras, 4 caracteristicas)\n",
      "y shape: (3, 3) (3 muestras, 3 clases)\n",
      "\n",
      "ARQUITECTURA DEL MODELO:\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esteb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ capa_oculta (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ capa_salida (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ capa_oculta (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                   │              \u001b[38;5;34m25\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ capa_salida (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                   │              \u001b[38;5;34m18\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">43</span> (172.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m43\u001b[0m (172.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">43</span> (172.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m43\u001b[0m (172.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "\n",
      "MODELO COMPILADO:\n",
      "  - Optimizador: SGD (learning_rate=0.5)\n",
      "  - Funcion de perdida: Categorical Crossentropy\n",
      "  - Metricas: Accuracy\n",
      "\n",
      "======================================================================\n",
      "PREDICCIONES ANTES DEL ENTRENAMIENTO (pesos aleatorios)\n",
      "======================================================================\n",
      "\n",
      "Predicciones (probabilidades):\n",
      "  Muestra 0: [0.0793648 0.6858896 0.2347456] -> Clase 1 | Real: Clase 0 | ✗\n",
      "  Muestra 1: [0.44278055 0.31846198 0.23875745] -> Clase 0 | Real: Clase 1 | ✗\n",
      "  Muestra 2: [0.03681113 0.5655071  0.39768174] -> Clase 1 | Real: Clase 2 | ✗\n",
      "\n",
      "Perdida inicial: 1.5334\n",
      "\n",
      "======================================================================\n",
      "ENTRENAMIENTO\n",
      "======================================================================\n",
      "\n",
      "Entrenamiento completado: 100 epocas\n",
      "Perdida final: 0.0172\n",
      "Accuracy final: 1.0000\n",
      "\n",
      "Evolucion de la perdida:\n",
      "  Epoca   1: Perdida = 1.5334\n",
      "  Epoca  20: Perdida = 0.1165\n",
      "  Epoca  40: Perdida = 0.0505\n",
      "  Epoca  60: Perdida = 0.0310\n",
      "  Epoca  80: Perdida = 0.0223\n",
      "  Epoca 100: Perdida = 0.0172\n",
      "\n",
      "======================================================================\n",
      "PREDICCIONES DESPUES DEL ENTRENAMIENTO\n",
      "======================================================================\n",
      "\n",
      "Predicciones (probabilidades):\n",
      "  Muestra 0: [9.9876606e-01 9.7932061e-04 2.5468459e-04] -> Clase 0 | Real: Clase 0 | ✓\n",
      "  Muestra 1: [0.02346364 0.9525032  0.02403314] -> Clase 1 | Real: Clase 1 | ✓\n",
      "  Muestra 2: [1.794985e-04 9.190401e-04 9.989015e-01] -> Clase 2 | Real: Clase 2 | ✓\n",
      "\n",
      "Perdida final: 0.0170\n",
      "Accuracy final: 1.0000\n",
      "\n",
      "======================================================================\n",
      "INSPECCION DE PESOS APRENDIDOS\n",
      "======================================================================\n",
      "\n",
      "capa_oculta:\n",
      "  Pesos shape: (4, 5)\n",
      "  Biases shape: (5,)\n",
      "  Pesos (primeras 3 filas):\n",
      "[[ 0.6582622   0.07442486 -0.56781924  0.18511449 -0.30909866]\n",
      " [-0.43205976 -0.726568    0.34109205 -0.03515198 -0.351795  ]\n",
      " [ 1.2159133   0.29993546  0.97677064 -0.1849911  -0.41836753]]\n",
      "  Biases: [ 0.05837462  0.          0.06967154 -0.03032274  0.        ]\n",
      "\n",
      "capa_salida:\n",
      "  Pesos shape: (5, 3)\n",
      "  Biases shape: (3,)\n",
      "  Pesos (primeras 3 filas):\n",
      "[[ 1.5068911  -0.3385572  -0.10737057]\n",
      " [-0.08690435  0.06230986 -0.65547794]\n",
      " [-0.16092215 -0.13910875  1.6283982 ]]\n",
      "  Biases: [-1.2425412  2.4611    -1.2185596]\n",
      "\n",
      "======================================================================\n",
      "COMPARACION: MANUAL vs TENSORFLOW\n",
      "======================================================================\n",
      "\n",
      "┌─────────────────────────┬──────────────────────┬─────────────────────────┐\n",
      "│ Componente              │ Implementacion Manual│ TensorFlow/Keras        │\n",
      "├─────────────────────────┼──────────────────────┼─────────────────────────┤\n",
      "│ Capa Densa              │ Clase Layer_Dense    │ layers.Dense()          │\n",
      "│ Activación ReLU         │ Clase Activation_ReLU│ activation='relu'       │\n",
      "│ Activación Softmax      │ Clase Activation_...│ activation='softmax'    │\n",
      "│ Función de pérdida      │ Clase Loss_...       │ loss='categorical_...'  │\n",
      "│ Optimizador             │ Clase Optimizer_SGD  │ optimizers.SGD()        │\n",
      "│ Forward pass            │ layer.forward(X)     │ modelo.predict(X)       │\n",
      "│ Cálculo de gradientes   │ Manual (backprop)    │ Automático              │\n",
      "│ Actualización de pesos  │ optimizer.update()   │ Automático              │\n",
      "│ Entrenamiento completo  │ Loop manual          │ modelo.fit()            │\n",
      "└─────────────────────────┴──────────────────────┴─────────────────────────┘\n",
      "\n",
      "LINEAS DE CODIGO:\n",
      "  - Manual:     ~150-200 líneas (todas las clases)\n",
      "  - TensorFlow: ~10 líneas (sin contar prints)\n",
      "\n",
      "VENTAJAS DE TENSORFLOW:\n",
      "  ✓ Código mucho más conciso\n",
      "  ✓ Gradientes calculados automáticamente\n",
      "  ✓ Optimizado para GPU/TPU\n",
      "  ✓ Menos propenso a errores\n",
      "  ✓ Fácil de escalar a redes más complejas\n",
      "\n",
      "VENTAJAS DE ENTENDER LA IMPLEMENTACION MANUAL:\n",
      "  ✓ Sabes qué hace cada componente\n",
      "  ✓ Puedes diagnosticar problemas\n",
      "  ✓ Entiendes las limitaciones\n",
      "  ✓ Puedes crear componentes personalizados\n",
      "  ✓ Tomas mejores decisiones de diseño\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RED NEURONAL CON TENSORFLOW/KERAS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================================\n",
    "# DATOS DE ENTRADA (los mismos que hemos usado)\n",
    "# ============================================================================\n",
    "X = np.array([[1.0, 2.0, 3.0, 2.5],\n",
    "              [2.0, 5.0, -1.0, 2.0],\n",
    "              [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "# Etiquetas para clasificación (3 clases)\n",
    "y_true = np.array([[1, 0, 0],  # Clase 0\n",
    "                   [0, 1, 0],  # Clase 1\n",
    "                   [0, 0, 1]]) # Clase 2\n",
    "\n",
    "print(\"\\nDatos de entrada:\")\n",
    "print(f\"X shape: {X.shape} (3 muestras, 4 caracteristicas)\")\n",
    "print(f\"y shape: {y_true.shape} (3 muestras, 3 clases)\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONSTRUCCIÓN DEL MODELO\n",
    "# ============================================================================\n",
    "# Fijar semilla para reproducibilidad\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Crear modelo secuencial (capa tras capa)\n",
    "modelo = keras.Sequential([\n",
    "    # Capa oculta: 4 entradas -> 5 neuronas con ReLU\n",
    "    layers.Dense(5, activation='relu', input_shape=(4,), name='capa_oculta'),\n",
    "    \n",
    "    # Capa de salida: 5 entradas -> 3 neuronas con Softmax\n",
    "    layers.Dense(3, activation='softmax', name='capa_salida')\n",
    "])\n",
    "\n",
    "print(\"ARQUITECTURA DEL MODELO:\")\n",
    "print(\"-\" * 70)\n",
    "modelo.summary()\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# ============================================================================\n",
    "# COMPILACIÓN DEL MODELO\n",
    "# ============================================================================\n",
    "# Aquí especificamos:\n",
    "# - Optimizador: SGD (Stochastic Gradient Descent)\n",
    "# - Función de pérdida: Categorical Crossentropy (para clasificación multiclase)\n",
    "# - Métricas: Accuracy (precisión)\n",
    "\n",
    "modelo.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nMODELO COMPILADO:\")\n",
    "print(\"  - Optimizador: SGD (learning_rate=0.5)\")\n",
    "print(\"  - Funcion de perdida: Categorical Crossentropy\")\n",
    "print(\"  - Metricas: Accuracy\")\n",
    "\n",
    "# ============================================================================\n",
    "# PREDICCIÓN ANTES DEL ENTRENAMIENTO\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PREDICCIONES ANTES DEL ENTRENAMIENTO (pesos aleatorios)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "y_pred_inicial = modelo.predict(X, verbose=0)\n",
    "print(\"\\nPredicciones (probabilidades):\")\n",
    "for i, pred in enumerate(y_pred_inicial):\n",
    "    clase_pred = np.argmax(pred)\n",
    "    clase_true = np.argmax(y_true[i])\n",
    "    print(f\"  Muestra {i}: {pred} -> Clase {clase_pred} | \"\n",
    "          f\"Real: Clase {clase_true} | {'✓' if clase_pred == clase_true else '✗'}\")\n",
    "\n",
    "# Calcular pérdida inicial\n",
    "loss_inicial = modelo.evaluate(X, y_true, verbose=0)[0]\n",
    "print(f\"\\nPerdida inicial: {loss_inicial:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# ENTRENAMIENTO\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ENTRENAMIENTO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Entrenar el modelo\n",
    "# epochs: número de veces que se pasa por todos los datos\n",
    "# verbose: controla cuánta información se imprime\n",
    "history = modelo.fit(\n",
    "    X, y_true,\n",
    "    epochs=100,\n",
    "    verbose=0  # Silencioso para no llenar la salida\n",
    ")\n",
    "\n",
    "print(f\"\\nEntrenamiento completado: {len(history.history['loss'])} epocas\")\n",
    "print(f\"Perdida final: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Accuracy final: {history.history['accuracy'][-1]:.4f}\")\n",
    "\n",
    "# Mostrar evolución cada 20 épocas\n",
    "print(\"\\nEvolucion de la perdida:\")\n",
    "for i in [0, 19, 39, 59, 79, 99]:\n",
    "    if i < len(history.history['loss']):\n",
    "        print(f\"  Epoca {i+1:3d}: Perdida = {history.history['loss'][i]:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PREDICCIÓN DESPUÉS DEL ENTRENAMIENTO\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PREDICCIONES DESPUES DEL ENTRENAMIENTO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "y_pred_final = modelo.predict(X, verbose=0)\n",
    "print(\"\\nPredicciones (probabilidades):\")\n",
    "for i, pred in enumerate(y_pred_final):\n",
    "    clase_pred = np.argmax(pred)\n",
    "    clase_true = np.argmax(y_true[i])\n",
    "    print(f\"  Muestra {i}: {pred} -> Clase {clase_pred} | \"\n",
    "          f\"Real: Clase {clase_true} | {'✓' if clase_pred == clase_true else '✗'}\")\n",
    "\n",
    "# Evaluar modelo\n",
    "loss_final, accuracy_final = modelo.evaluate(X, y_true, verbose=0)\n",
    "print(f\"\\nPerdida final: {loss_final:.4f}\")\n",
    "print(f\"Accuracy final: {accuracy_final:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# INSPECCIÓN DE PESOS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"INSPECCION DE PESOS APRENDIDOS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, layer in enumerate(modelo.layers):\n",
    "    weights, biases = layer.get_weights()\n",
    "    print(f\"\\n{layer.name}:\")\n",
    "    print(f\"  Pesos shape: {weights.shape}\")\n",
    "    print(f\"  Biases shape: {biases.shape}\")\n",
    "    print(f\"  Pesos (primeras 3 filas):\\n{weights[:3]}\")\n",
    "    print(f\"  Biases: {biases}\")\n",
    "\n",
    "# ============================================================================\n",
    "# COMPARACIÓN CON IMPLEMENTACIÓN MANUAL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPARACION: MANUAL vs TENSORFLOW\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "comparacion = \"\"\"\n",
    "┌─────────────────────────┬──────────────────────┬─────────────────────────┐\n",
    "│ Componente              │ Implementacion Manual│ TensorFlow/Keras        │\n",
    "├─────────────────────────┼──────────────────────┼─────────────────────────┤\n",
    "│ Capa Densa              │ Clase Layer_Dense    │ layers.Dense()          │\n",
    "│ Activación ReLU         │ Clase Activation_ReLU│ activation='relu'       │\n",
    "│ Activación Softmax      │ Clase Activation_...│ activation='softmax'    │\n",
    "│ Función de pérdida      │ Clase Loss_...       │ loss='categorical_...'  │\n",
    "│ Optimizador             │ Clase Optimizer_SGD  │ optimizers.SGD()        │\n",
    "│ Forward pass            │ layer.forward(X)     │ modelo.predict(X)       │\n",
    "│ Cálculo de gradientes   │ Manual (backprop)    │ Automático              │\n",
    "│ Actualización de pesos  │ optimizer.update()   │ Automático              │\n",
    "│ Entrenamiento completo  │ Loop manual          │ modelo.fit()            │\n",
    "└─────────────────────────┴──────────────────────┴─────────────────────────┘\n",
    "\n",
    "LINEAS DE CODIGO:\n",
    "  - Manual:     ~150-200 líneas (todas las clases)\n",
    "  - TensorFlow: ~10 líneas (sin contar prints)\n",
    "\n",
    "VENTAJAS DE TENSORFLOW:\n",
    "  ✓ Código mucho más conciso\n",
    "  ✓ Gradientes calculados automáticamente\n",
    "  ✓ Optimizado para GPU/TPU\n",
    "  ✓ Menos propenso a errores\n",
    "  ✓ Fácil de escalar a redes más complejas\n",
    "\n",
    "VENTAJAS DE ENTENDER LA IMPLEMENTACION MANUAL:\n",
    "  ✓ Sabes qué hace cada componente\n",
    "  ✓ Puedes diagnosticar problemas\n",
    "  ✓ Entiendes las limitaciones\n",
    "  ✓ Puedes crear componentes personalizados\n",
    "  ✓ Tomas mejores decisiones de diseño\n",
    "\"\"\"\n",
    "\n",
    "print(comparacion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1822b5e-d187-42d2-b4c9-f704c132ca5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
